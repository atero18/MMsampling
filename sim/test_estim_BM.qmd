---
title: "measure bias correction (linear case)"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

```{r}
# knitr::purl("./sim/test_estim.qmd")
```

```{r}
#| message: false
#| warning: false
library(dplyr)
options(dplyr.summarise.inform = FALSE)
library(tibble)
library(tidyr)
library(glue)
library(tibble)
library(parallel)
library(tictoc)
library(ggplot2)
theme_set(theme_light())
library(Matrix)
library(progress)
library(marginaleffects)
library(progress)
library(mc2d)
```

```{r}
#| message: false
#| warning: false
devtools::load_all()
```

```{r}
#| echo: false
#| eval: false
load(file = "../results.RData")
```

Linear models on both modes

# Building constants

```{r}
RNGkind(kind = "L'Ecuyer-CMRG")
set.seed(145L)
```

```{r}
N <- 10000L
phi <- rep(0.5, N)
```

Creating independent covariates

```{r}
p <- 1L

sex <- sample(c(0L, 1L), replace = TRUE, size = N, prob = c(0.5, 0.5))
age <- rnorm(n = N, mean = sex * 41.1 + (1L - sex) * 43.9, sd = 10.0)
age[age < 0.0] <- 0.0

X <- Z <- data.frame(const = 1.0, 
                     age = age, 
                     sex = sex)

rm(age)
sex <- factor(sex)
levels(sex) <- c("Woman", "Man")
```

```{r}
summary(X[, -1L])
```

```{r}
data <- X[, c("sex", "age")]
data$sex <- sex
ggplot(data) + 
  geom_density(aes(x = age, colour = sex)) +
  ggtitle("Age density conditionally to the sex")

data %>% group_by(sex) %>% summarize(meanAge = mean(age))
rm(data)
```

```{r}
X <- Z <- as.matrix(X)
```

Creating parameters for two modes : "int" and "tel". `delta` is defined such that the average measure effect is equal to 1. It depends on `X`.

```{r}
betaTel <- c(1.0, -0.1, 4.0)
print(betaTel)
delta <- c(1.0, -2.0 * mean(X[, "age"])^-1L, 2.0 * N / sum(X[, "sex"]))
betaInt <- betaTel + delta

print(betaInt)
```

$Y_{int}$ and $Y_{tel}$ expectations depending on `X`:

```{r}
data.frame(tel = X %*% betaTel, int = X %*% betaInt) %>% 
  pivot_longer(cols = c("tel", "int"), names_to = "mode", values_to = "value") %>% 
  ggplot() + 
  geom_density(aes(x = value, colour = mode)) +
  ggtitle("Distribution of the counterfactuals expected values")

glue("average value for telephone: {mean(X %*% betaTel)}")
glue("average value for internet: {mean(X %*% betaInt)}")
```

With telephone counterfactuals only:
```{r}
data.frame(tel = X %*% betaTel) %>% 
  ggplot() + 
  geom_density(aes(x = tel)) +
  # ggtitle("Distribution of the expected value of telephone outcomes") +
  ggtitle("Distribution of the expected value of telephone outcomes") +
  xlab("expected value")
```


```{r}
as.numeric(X %*% betaTel) %>% summary()
```

```{r}
totBias <- sum(X %*% delta)
glue("total bias on U: {totBias}")
meanBias <- totBias / N
glue("average bias on U: {meanBias}")
```

We simulate a mode selection MAR (Missing At Random) under a logistic model, with $X$ as variable:

```{r}
# With MAR

expit <- function(x) 1.0 / (1.0 + exp(-x))

alphaInt <- c(0.7, -0.02, -0.5)
pInt <- expit(X %*% alphaInt) %>% as.vector()

alphaTel <- c(0.3, 0.02, -0.5)
pTel <- expit(X %*% alphaTel) %>% as.vector()
```

Here are some details about the $p_{int}$ and $p_{tel}$:

```{r}
glue("internet probabilities:\n")
summary(pInt) %>% print()
glue("telephone probabilities:\n")
summary(pTel) %>% print()
```


```{r}
pRepTel <- (1.0 - pInt) * pTel
summary(pRepTel)
```

About $S_{r\bullet}$ (case of SRS with $f = \frac{1}{10}$):

```{r}
f <- 1.0 / 10.0
n <- f * N
expSizeSr <- f * sum(pInt)

pi <- rep(f, N)
piMat <- matrix(n * (n - 1L) / (N * (N - 1L)), nrow = N, ncol = N)
diag(piMat) <- f
pCov <- pi2_to_covarInc(piMat)
expVarSr <- f * sum(pInt * (1.0 - pInt)) + 
  t(pInt) %*% pCov %*% pInt %>% 
  as.numeric()

expSizeSr
expVarSr
```
About $S_{mr}$ (case of SRS with $f = \frac{1}{10}$):

```{r}
expSizeSmr <- f * sum(pRepTel)

expVarSmr <- f * sum(pRepTel * (1.0 - pTel)) + 
  f * sum(pRepTel * (1.0 - pInt)) +
  t(pRepTel) %*% pCov %*% pRepTel %>% 
  as.numeric()

expSizeSmr
expVarSmr
```

About $S_a$ (case of SRS with $f = \frac{1}{10}$):

```{r}
expSizeSa <- expSizeSr + expSizeSmr

expVarSa <- f * sum(pRepTel * (1.0 - pTel)) + 
  f * sum(pInt * (1.0 - pInt) * (1.0 - pTel)) +
  t(pInt + pRepTel) %*% pCov %*% (pInt + pRepTel) %>% 
  as.numeric()

expSizeSa
expVarSa
```

```{r}
rm(expSizeSr, expVarSr, 
   expSizeSmr, expVarSmr, 
   expSizeSa, expVarSa)
```


```{r}
data.frame(mode = factor(rep(c("web", "telephone"), each = N)), 
           prob = c(pInt, pTel)) %>% 
  ggplot() + 
  geom_density(aes(x = prob, color = mode)) +
  ggtitle("Distribution of p1 and p2") +
  xlab("mode selection probability")
```

```{r}
pInt[sex == "Man"] %>% summary()
pInt[sex == "Woman"] %>% summary()
```

# Simulation functions

Randomly affect a mode (or non-response) to each unit

```{r}
gen_choice_bimode <- function(I, p1, p2, checkRank = FALSE, X = NULL)
{
  N <- length(p1)
  
  R1 <- runif(N) <= p1
  R2 <- runif(N) <= p2
  
  modes <- rep("nr", N)
  
  modes[I & R1] <- "int"
  modes[I & !R1 & R2] <- "tel"
  
  q <- ncol(Z)
  
  # While the rank for the submatrices are not enough
  # we rerun the simulation
  if (checkRank &&
      (rankMatrix(Z[R1, , drop = FALSE]) < q || 
      rankMatrix(Z[!R1 & R2, , drop = FALSE]) < q))
  {
    warning("Rank for submatrices are not enough")
    return(gen_choice_bimode(I, p1, p2, checkRank = TRUE, Z))
  }

  # probSelect : probability of selecting the chosen mode
  # if selected in sample S
  probSelect <- rep(NA_real_, N)
  
  probSelect[R1] <- p1[R1]
  probSelect[!R1 & R2] <- (1.0 - p1[!R1 & R2]) * p2[!R1 & R2]
  
  tibble(p1 = p1, r1 = R1, p2 = p2, r2 = R2, 
         mode = modes, probSelect = probSelect)
}
```

```{r}
gen_expY <- function(X, 
                     betaInt, YintLaw = "gaussian", sdInt,
                     betaTel, YtelLaw = "gaussian", sdTel)
{
  if (YtelLaw == "gaussian")
  {

    expYtels <- as.vector(X %*% betaTel)
    
    sdTel <- as.numeric(sdTel)
    sdTels <- rep(sdTel, N)
    # covarYtel <- diag(sdTels^2L)
      
  }
  else if (YtelLaw == "exponential")
  {
    sdTel <- "null"

    expYtels <- as.vector(abs(X %*% betaTel))
    
    sdTels <- expYtels
    
    # covarYtel <- diag(sdTels^2L)
  }
  
  
  if (YintLaw == "gaussian")
  {
    
    expYints <- as.vector(X %*% betaInt)
    
    sdInt <- as.numeric(sdInt)
    sdInts <- rep(sdInt, N)
    # covarYint <- diag(sdInts^2L)
    
  }
  
  else if (YintLaw == "exponential")
  {
    sdInt <- "null"
    

    expYints <- as.vector(abs(X %*% betaInt))
    
    sdInts <- expYints
    # covarYint <- diag(sdInts^2L)
  }
  else if (YintLaw == "split3")
  {
    
    if (is.null(expYints))
      expYints <- as.vector(abs(X %*% betaInt))
    
    quantilesInt <- quantile(expYints, probs = c(1.0 / 3.0, 2.0 / 3.0))
    
    sdInt <- as.numeric(sdInt)
    sdInts <- rep(as.numeric(sdInt), N)
    # covarYint <- diag(sdInts^2L)
    
    # Between first and second quantile of 3rd degree we add one
    expYints[expYints > quantilesInt[1L]] <- 
      expYints[expYints > quantilesInt[1L]] + 1.0
    
    # After the second quantile of 3rd degree we add one more
    expYints[expYints > quantilesInt[2L]] <- 
      expYints[expYints > quantilesInt[2L]] + 1.0
  }
  
  list(expYints = expYints, sdInts = sdInts,
       expYtels = expYtels, sdTels = sdTels)
  
}
```

```{r}
gen_Y <- function(X, 
                  betaInt, sigmaInt, YintLaw = "gaussian",
                  betaTel, sigmaTel = sigmaInt, YtelLaw = "gaussian",
                  expYints = NULL, expYtels = NULL,
                  rho = 0.0)
{
  
  N <- nrow(X)
  
  recExpY <- is.null(expYints) || is.null(expYtels)
  
  if (recExpY)
  {
    dataY <- gen_expY(X, 
                      betaInt, YintLaw, sigmaInt,
                      betaTel, YtelLaw, sigmaTel)
    
    expYints <- dataY$expYints
    sdInts <- dataY$sdInts
    
    expYtels <- dataY$expYtels
    sdTels <- dataY$sdTels
  }
  else
  {
    if (YtelLaw == "gaussian")
      sdTels <- rep(sigmaTel, N)
    else if (YtelLaw == "exponential")
      sdTels <- expYtels
    
    if (YintLaw == "gaussian")
      sdInts <- rep(sigmaInt, N)
    else if (YintLaw == "exponential")
      sdInts <- expYints
  }
  
  
  # Affecting values to Y_int and Y_tel
  if (YtelLaw == "gaussian")
    Ytels <- expYtels + rnorm(n = N, sd = sdTels)
    
  else if (YtelLaw == "exponential")
    Ytels <- rexp(n = N, rate = expYtels^-1L)
  
  if (YintLaw %in% c("gaussian", "split3"))
    Yints <- expYints + rnorm(n = N, sd = sdInts)

  else if (YintLaw == "exponential")
    Yints <- rexp(n = N, rate = expYints^-1L)
  
  
  
  
  if (rho != 0.0)
  {
    results <- cbind(Yint = Yints, Ytel = Ytels) %>% 
      mc2d::cornode(target = rho, result = FALSE) %>% 
      as_tibble()
  }
  else
    results <- tibble(Yint = Yints, Ytel = Ytels)
  
  if (recExpY)
  {
    results <- results %>%
    mutate(expYint = expYints, .before = "Yint") %>% 
    mutate(expYtel = expYtels, .before = "Ytel")
  }
  
  results
}
```

```{r}
set_Yobs <- function(Yint, Ytel, modes)
{
  N <- length(Yint)
  
  Yobs <- rep(NA_real_, N)
  Yobs[modes == "int"] <- Yint[modes == "int"]
  Yobs[modes == "tel"] <- Ytel[modes == "tel"]
  
  Yobs
}
```

```{r}
set_phi <- function(phi = "eq", N)
{
  if (phi == "eq")
    phi <- rep(0.5, N)
  
  else if (phi == "1/3")
    phi <- rep(1.0 / 3.0, N)
  
  else if (phi == "2/3")
    phi <- rep(2.0 / 3.0, N)

  else if (phi == "var")
    phi <- seq_len(N) / N
  
  # When there is no bias, phi = 0.5
  # When the bias increase (in absolute), phi tends to zero
  # (it gives more weights to the unbiased value)
  else if (phi == "linear")
  {
    expDeltas <- expYints - expYtels
    maxBias <- max(abs(expDeltas))
    phi <- 0.5 - 0.5 * abs(expDeltas) / maxBias
  }
  else if (phi == "split3")
  {
    expDeltas <- expYints - expYtels
    absExpDeltas <- abs(expDeltas)
    quantiles <- quantile(absExpDeltas, c(1.0 / 3.0, 2.0 / 3.0))
    phi <- rep(0.5, N)
    phi[absExpDeltas >= quantiles[1L]] <- 1.0 / 3.0
    phi[absExpDeltas >= quantiles[2L]] <- 1.0 / 6.0
  }
    
  phi
}
```

```{r}
piMat_SRS <- function(n, N)
{
  piMat <- matrix(n * (n - 1L) / (N * (N - 1L)), nrow = N, ncol = N)
  diag(piMat) <- n / N
  
  piMat
}
```

```{r}
covar_SRS <- function(n, N)
{
  piMat_SRS(n, N) %>% pi2_to_covarInc()
}
```



Parameters:

-   $\alpha_{tel}$ and $\alpha_min$, coefficients of conditional expectations of $r_{tel}$ and $r_{int}$
-   $\beta_{tel}$ and $\beta_{int}$, coefficients of conditional expectations of $Y_{tel}$ and $Y_{int}$
-   the sampling type (`sampling`)
-   the conditional standard deviations of $Y_{tel}$ and $Y_{int}$ (`sdInt` and `sdTel`)
-   the law of $Y_{tel}$ (`YtelLaw`)
-   the law of $Y_{int}$ (`YintLaw`)
-   the state of $\phi$ (`phi`)
-   the number of iterations (`M`)
-   the random seed (`seed`)

```{r}
simulationUncounf <- 
  function(N, 
           sampling = "SRS",
           alphaInt, alphaTel, 
           betaInt, betaTel, 
           YintLaw = "gaussian", YtelLaw = "gaussian",
           sdInt = 1.0, sdTel = 1.0, 
           rho = 0.8,
           phi = rep(0.5, N), M = 1000L,
           X = NULL,
           n = ceiling(N / 10L),
           seed = 123L,
           cluster = NULL)
  {
    
    stopClusterAtEnd <- is.null(cluster)
    # X is exported only if the cluster is created in the function
    if (is.null(cluster))
    {
      nbCores <- detectCores() - 1L
      cluster <- makeCluster(nbCores)
    
      clusterExport(cluster, 
                    varlist = "X",
                    envir = environment())
    }
    
    
    # Making clusters random-independents
    clusterSetRNGStream(cl = cluster, iseed = seed)
    
    clusterEvalQ(cluster, library(dplyr))
    clusterEvalQ(cluster, library(Matrix))
    clusterEvalQ(cluster, library(tibble))
    clusterEvalQ(cluster, library(marginaleffects))
    clusterEvalQ(cluster, library(MMsampling))
    
    clusterExport(cluster, varlist = c("sampling", 
                                       "betaInt", "betaTel", 
                                       "YintLaw", "YtelLaw", 
                                       "sdInt", "sdTel", 
                                       "n"),
                  envir = environment())
    
    expit <- function(x) 1.0 / (1.0 + exp(-x))

    pInt <- expit(X %*% alphaInt) %>% as.numeric()
    pq1Mat <- pInt %*% t(pInt)
    diag(pq1Mat) <- pInt
    
    pTel <- expit(X %*% alphaTel) %>% as.numeric()
    pq2Mat <- pTel %*% t(pTel)
    diag(pq2Mat) <- pTel
    
    clusterExport(cluster, 
                  varlist = c("pInt", "pq1Mat", "pTel", "pq2Mat"),
                  envir = environment())
    

    if (sampling == "SRS")
    {
      pi <- rep(n / N, N)
      piMat <- piMat_SRS(n, N)
      
      clusterExport(cluster, 
                    varlist = c("pi", "piMat"),
                    envir = environment())
    }
    
    
    dataExpY <- gen_expY(X, 
                         betaInt, YintLaw, sdInt,
                         betaTel, YtelLaw, sdTel)
    
    expYints <- dataExpY$expYints
    sdInts <- dataExpY$sdInts
    
    expYtels <- dataExpY$expYtels
    sdTels <- dataExpY$sdTels
    
    rm(dataExpY)
    
    
    # if (YtelLaw == "gaussian")
    # {
    #   expYtels <- as.vector(Z %*% betaTel)
    # }
    # else if (YtelLaw == "exponential")
    # {
    #   sdTel <- "null"
    #   expYtels <- as.vector(abs(X %*% betaTel))
    # }
    # 
    # expTotTel <- sum(expYtels)
    # 
    # if (YintLaw == "gaussian")
    #   expYints <- as.vector(X %*% betaInt)
    #   
    # else if (YintLaw == "exponential")
    # {
    #   sdInt <- "null"
    #   expYints <- as.vector(abs(X %*% betaInt))
    # }
    # else if (YintLaw == "split3")
    # {
    #   expYints <- as.vector(X %*% betaInt)
    #   quantilesInt <- quantile(expYints, probs = c(1.0 / 3.0, 2.0 / 3.0))
    #   
    # 
    #   sdInt <- as.numeric(sdInt)
    #   sdInts <- rep(as.numeric(sdInt), N)
    #   covarYint <- diag(sdInts^2L)
    #   
    #   # Between first and second quantile of 3rd degree we add one
    #   expYints[expYints > quantilesInt[1L]] <- 
    #     expYints[expYints > quantilesInt[1L]] + 1.0
    #   
    #   # After the second quantile of 3rd degree we add one more
    #   expYints[expYints > quantilesInt[2L]] <- 
    #     expYints[expYints > quantilesInt[2L]] + 1.0
    # }
    
    expTotYtel <- sum(expYtels)
    expTotYint <- sum(expYints)
    
    clusterExport(cluster,
                  varlist = c("expYtels", "expTotYtel",
                              "expYints", "expTotYint",
                              "sdInts", "sdTels", "rho"),
                  envir = environment())
    
    clusterExport(cluster, c("gen_choice_bimode", 
                  "gen_expY", "gen_Y", "set_Yobs", "set_phi"))
      
    
    if (class(phi) == "character")
      phiType <- phi
    else
      phiType <- "vector"
    
    phi <- set_phi(phi, N)
    
    # if (class(phi) == "character")
    #   phiType <- phi
    # else
    #   phiType <- "vector"
    # 
    # if (phi == "eq")
    #   phi <- rep(0.5, N)
    # 
    # else if (phi == "1/3")
    #   phi <- rep(1.0 / 3.0, N)
    # 
    # else if (phi == "2/3")
    #   phi <- rep(2.0 / 3.0, N)
    # 
    # else if (phi == "var")
    #   phi <- seq_len(N) / N
    
    # # When there is no bias, phi = 0.5
    # # When the bias increase (in absolute), phi tends to zero
    # # (it gives more weights to the unbiased value)
    # else if (phi == "linear")
    # {
    #   expDeltas <- expYints - expYtels
    #   maxBias <- max(abs(expDeltas))
    #   phi <- 0.5 - 0.5 * abs(expDeltas) / maxBias
    # }
    # else if (phi == "split3")
    # {
    #   expDeltas <- expYints - expYtels
    #   absExpDeltas <- abs(expDeltas)
    #   quantiles <- quantile(absExpDeltas, c(1.0 / 3.0, 2.0 / 3.0))
    #   phi <- rep(0.5, N)
    #   phi[absExpDeltas >= quantiles[1L]] <- 1.0 / 3.0
    #   phi[absExpDeltas >= quantiles[2L]] <- 1.0 / 6.0
    # }
      
    
    clusterExport(cluster, 
                  varlist = "phi", 
                  envir = environment())
    
    monoSim <- function(...)
    {
      nbExperiments <- 12L
      
      partialResults <- data.frame(parameter = character(nbExperiments),
                                   trueEstimator = logical(nbExperiments),
                                   probSelect = character(nbExperiments),
                                   invMatrices = character(nbExperiments),
                                   calculTotal = character(nbExperiments),
                                   estPhiBias = numeric(nbExperiments))
      
      row <- 1L
      
      Z <- X
      p <- q <- ncol(X)
        
      
      if (sampling == "SRS")
      {
        selectedSample <- sample(seq_len(N), size = n, replace = FALSE)
        
        I <- logical(N)
        I[selectedSample] <- TRUE
        rm(selectedSample)
      }
      
      # Mode selection simulation
      # modeChoiceOK <- FALSE
      # 
      # while (!modeChoiceOK)
      # {
      #   # modeChoiceOK <- TRUE
      #   
      #   dataModes <- gen_choice_bimode(I, pInt, pTel)
      #   modes <- dataModes$mode
      #   maskInt <- modes == "int"
      #   nInt <- sum(maskInt)
      #   maskTel <- modes == "tel"
      #   nTel <- sum(maskTel)
      #   trueProbsSelect <- dataModes$probSelect
      #   
      #   if (rankMatrix(Z[maskInt, , drop = FALSE]) == q &&
      #       rankMatrix(Z[maskTel, , drop = FALSE]) == q)
      #     modeChoiceOK <- TRUE
      #   else
      #     warning("Singular Zint / Ztel")
      #   
      # }
      
      dataModes <- gen_choice_bimode(I, pInt, pTel)
      modes <- dataModes$mode
      
      maskInt <- modes == "int"
      nInt <- sum(maskInt)
      maskTel <- modes == "tel"
      nTel <- sum(maskTel)
      
      trueProbsSelect <- dataModes$probSelect
      
      rm(dataModes)
      
      trueWeights <- (pi * trueProbsSelect)^-1L
    
      # Estimation of mode selection probabilities
      estimProbsSelect <- 
        MMsampling::estim_response_prob_sequential(I, pi, Z,
                                                   modes, 
                                                   c("int", "tel"))$unconditional
    
      estimWeights <- (pi * estimProbsSelect)^-1L
      
      # Generating values for Y_int and Y_tel
      dataY <- gen_Y(X, 
                     betaInt, sdInt, YintLaw,
                     betaTel, sdTel, YtelLaw,
                     expYints = expYints, expYtels = expYtels,
                     rho = rho)
      
      Yints <- dataY$Yint
      Ytels <- dataY$Ytel
      
      rm(dataY)
      
      # Creating the observed outcome vector
      Yobs <- set_Yobs(Yint = Yints, Ytel = Ytels, modes = modes)
      
      #  # Affecting values to Y_int and Y_tel
      # if (YtelLaw == "gaussian")
      # {
      #   Ytels <- expYtels + rnorm(n = N, sd = sdTels)
      # }
      #   
      # 
      # else if (YtelLaw == "exponential")
      #   Ytels <- rexp(n = N, rate = expYtels^-1L)
      # 
      # expTotYtel <- sum(expYtels)
      # trueYtel <- sum(Ytels)
      # 
      # if (YintLaw %in% c("gaussian", "split3"))
      #   Yints <- expYints + rnorm(n = N, sd = sdInts)
      # 
      # else if (YintLaw == "exponential")
      #   Yints <- rexp(n = N, rate = expYints^-1L)
      
      trueYtel <- sum(Ytels)
      deltas <- Yints - Ytels
      
      # Evaluating the measure effect total (random value)
      truePhiBias <- crossprod(phi, deltas) %>% as.vector()
      expPhiBias <- crossprod(phi, expYints - expYtels) %>% as.vector()
      
      # Creating the observed outcome vector
      # Yobs <- rep(NA_real_, N)
      # Yobs[maskInt] <- Yints[maskInt]
      # Yobs[maskTel] <- Ytels[maskTel]
      
      sample <- MMsampling:::MMSample$new(Z = X, pi = pi, I = I, 
                             modes = modes, Yobs = Yobs, phi = phi)
      
      # Full HT estimation, with full population matrix (X^tX)^-1 and
      # known probabilities
      resEval <-
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes,
                               "int", "tel", "HT", "HT", pi,
                        trueProbsSelect, sampleMatrix = FALSE) %>%
        estim_MB_by_MCO(X, phi = phi) %>%
        sum()
      
  
      partialResults[row,] <- 
        c(parameter = "HT", trueEstimator = FALSE,
          probSelect = "true", invMatrices = "population",
          calculTotal = "population", estPhiBias = resEval)
      
      row <- row + 1L
  
      # Full HT estimation, with sample matrix (Z_S^TZ_S)^-1 and
      # known probabilities
      resEval <-
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes,
                               "int", "tel", "HT", "HT", pi,
                        trueProbsSelect, sampleMatrix = TRUE) %>%
        estim_MB_by_MCO(Z, phi = phi) %>%
        sum()
  
      partialResults[row,] <- c(parameter = "HT", trueEstimator = FALSE,
                                probSelect = "true", invMatrices = "samples", 
                                calculTotal = "population", estPhiBias = resEval)
      
      row <- row + 1L
      

      # Estimation on a unique MCO for Y1 and Y2, with beta X and only a constant
      # delta for the estimation of the measure effect
      deltaEval <- 
        estim_delta_MCO_unique_model_const(Z, Yobs, modes, "int", "tel")
      resEvalFull <- sum(phi) * deltaEval
      resEvalHT <- 
        as.vector(crossprod(phi[maskInt],
                            estimWeights[maskInt]) * 
                    deltaEval)
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_0_deg",
                   trueEstimator = TRUE,
                   probSelect = "null",
                   invMatrices = "null",
                   calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      
      row <- row + 2L
      
      
      # G-COMP first degree estimation, <=> double MCO estimation
      # tic()
      # estimdelta <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
      #                   "int", "tel", "MCO", "MCO", pi, NULL)
      estMBs <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                                "int", "tel", "G-COMP", "G-COMP", pi, NULL, 
                                returnMB = TRUE, order = 1L)
      
      resEvalFull <- crossprod(phi, estMBs) %>% as.vector()
      
      resEvalHT <- crossprod(phi[maskInt] * estimWeights[maskInt],
                             estMBs[maskInt]) %>% as.vector()
      # toc() %>% print()
      
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_1_deg", trueEstimator = TRUE,
                   probSelect = "null", 
                   invMatrices = "null", calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # G-COMP two degrees estimation
      # tic()
      estMBs <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                                "int", "tel", "G-COMP", "G-COMP", pi, NULL, 
                                returnMB = TRUE, order = 2L)
      
      resEvalFull <- crossprod(phi, estMBs) %>% as.vector()
      
      resEvalHT <- crossprod(phi[maskInt] * estimWeights[maskInt],
                             estMBs[maskInt]) %>% as.vector()
      # toc() %>% print()
      
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_2_deg", trueEstimator = TRUE,
                   probSelect = "null", 
                   invMatrices = "null", calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      
      
      # Full HT estimation, with full population matrix (Z^tZ)^-1 and
      # unknown probabilities
      # tic()
      estimdelta <- 
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                        "int", "tel", "HT", "HT", pi, 
                        estimProbsSelect, sampleMatrix = FALSE)
      
      resEvalFull <- estimdelta %>% 
        estim_MB_by_MCO(Z, phi = phi) %>% 
        sum()
      
      resEvalHT <- estimdelta %>% 
        estim_MB_by_MCO(Z, 
                        phi = phi, 
                        weights = estimWeights,
                        mask = maskInt) %>% 
        sum()
    
      # toc() %>% print()
     
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "HT", trueEstimator = TRUE,
                   probSelect = "estimation", invMatrices = "population", 
                   calculTotal = c("population", "sample"), 
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # Full HT estimation, with sample matrix (Z_S^TZ_S)^-1 and
      # unknown probabilities
      # tic()
      estimdelta <- 
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                        "int", "tel", "HT", "HT", pi, 
                        estimProbsSelect, sampleMatrix = TRUE) 
      
      
      resEvalFull <- estimdelta %>% 
        estim_MB_by_MCO(Z, phi = phi) %>% 
        sum()
      
      resEvalHT <- estimdelta %>% 
        estim_MB_by_MCO(Z, 
                        phi = phi, 
                        weights = estimWeights, 
                        mask = maskInt) %>% 
        sum()
    
      # toc() %>% print()
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "HT", trueEstimator = TRUE,
                   probSelect = "estimation", invMatrices = "samples", 
                   calculTotal = c("population", "sample"), 
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # Estimations of t_phi1 and t_phi2
      # - With true weights
      estTotPhiYintHTTrueMP <- 
        phi[maskInt] * 
        trueWeights[maskInt] * 
        Yints[maskInt]
      
      estTotPhiYintHTTrueMP <- sum(estTotPhiYintHTTrueMP)
      
      estTotPhiYtelHTTrueMP <- 
        (1.0 - phi[maskTel]) * 
        trueWeights[maskTel] * 
        Ytels[maskTel]
      
      # - With estimated weights
      estTotPhiYtelHTTrueMP <- sum(estTotPhiYtelHTTrueMP)
    
      estTotPhiYintHTEstMP <-
        phi[maskInt] *
        estimWeights[maskInt] *
        Yints[maskInt]
    
      estTotPhiYintHTEstMP <- sum(estTotPhiYintHTEstMP)
    
      estTotPhiYtelHTEstmMP <-
        (1.0 - phi[maskTel]) *
        estimWeights[maskTel] *
        Ytels[maskTel]
    
      estTotPhiYtelHTEstmMP <- sum(estTotPhiYtelHTEstmMP)
      
      estPhiYtelEstMP <-
        phi[maskTel] *
        estimWeights[maskTel] *
        Ytels[maskTel]
      
      estPhiYtelEstMP <- sum(estPhiYtelEstMP)
      
      normEstPhiBiasDiffHT <-
        (estTotPhiYintHTEstMP - estPhiYtelEstMP) / sum(phi)
      
      # Don't know why but some columns are
      #  automatically converted as character
      partialResults$estPhiBias <- as.numeric(partialResults$estPhiBias)
      partialResults$trueEstimator <- as.logical(partialResults$trueEstimator)
      # Value estimated only on telephone answers, with estimated selection probabilities
      benchmark <- crossprod(estimWeights[maskTel], Ytels[maskTel]) %>% 
        as.vector()
      trueWeightsBenchmark <- crossprod(trueWeights[maskTel], Ytels[maskTel]) %>% 
        as.vector()

      
      partialResults <- partialResults %>% 
        add_column(.before = 1L, expn = sum(pi)) %>% 
        add_column(.after = "expn", n = sum(I)) %>% 
        add_column(.after = "n", nInt = nInt) %>% 
        add_column(.after = "nInt", nTel = nTel) %>% 
        add_column(.after = "nTel", expYint = expTotYint) %>% 
        add_column(.after = "expYint", trueYtel = trueYtel) %>% 
        add_column(.after = "trueYtel", expYtel = expTotYtel) %>% 
        add_column(.before = "estPhiBias", truePhiBias = truePhiBias) %>% 
        add_column(.adter = "estPhiBias", normEstPhiBiasDiffHT = normEstPhiBiasDiffHT) %>% 
        add_column(.after = "truePhiBias", expPhiBias = expPhiBias) %>% 
        add_column(.after = "expPhiBias", 
                   estPhiYintTrueMP = estTotPhiYintHTTrueMP) %>% 
        add_column(.after = "estPhiYintTrueMP", avgEstWeightsInt = mean(trueWeights[maskInt])) %>% 
        add_column(.after = "estPhiYintTrueMP", 
                   estPhiYintEstMP = estTotPhiYintHTEstMP) %>% 
        add_column(.after = "estPhiYintEstMP", avgEstWeightsTel = mean(trueWeights[maskTel])) %>%
        add_column(.after = "avgEstWeightsTel", estPhiYtelEstMP = estPhiYtelEstMP) %>% 
        add_column(.after = "estPhiYtelEstMP", 
                   estPhiBarYtelTrueMP = estTotPhiYtelHTTrueMP) %>% 
        add_column(.after = "estPhiBarYtelTrueMP", 
                   estPhiBarYtelEstMP = estTotPhiYtelHTEstmMP) %>% 
        mutate(.after = "estPhiBarYtelEstMP",
               estYtelTrueMP = estPhiYintTrueMP + 
                 estPhiBarYtelTrueMP -
                 estPhiBias) %>% 
        mutate(.after = "estYtelTrueMP", 
               estYtelEstMP = estPhiYintEstMP + 
                 estPhiBarYtelEstMP -
                 estPhiBias) %>% 
        add_column(.after = "estYtelEstMP", benchmark = benchmark) %>% 
        add_column(.after = "benchmark", TWBenchmark = trueWeightsBenchmark)
        
      #results <- rbind(results, partialResults)
    }
    
    clusterExport(cluster, 
                  varlist = "monoSim", 
                  envir = environment())
    
    #results <- lapply(X = seq_len(M), FUN = monoSim)
    results <- parLapply(cluster, X = seq_len(M), fun = monoSim)
    
    if (stopClusterAtEnd)
      stopCluster(cluster)
      

    parameters <- results[[1L]] %>% 
    select(parameter, probSelect, invMatrices, calculTotal)
    
    parameters[, c("expVar2", "expVarPhi1", "expVarPhi2", 
                   "expCovarPhi12", "expVarPhiDelta", 
                   "expCovarPhi1Delta", "expCovarPhi2Delta")] <- NA_real_
  
    for (l in seq_len(nrow(parameters)))
    {
      parameter <- parameters[l, ]
      
      if (parameter$parameter == "HT" && 
          parameter$probSelect == "true" &&
          parameter$invMatrices == "population" && 
          parameter$calculTotal == "population")
      {
        expVars <- var_estim_tot_BM(modeTotBiased = "HT", modeTotRef = "HT",
                                    calculTotal = "population",
                                    expY1 = expYints, expY2 = expYtels,
                                    covarY1 = covarYint, covarY2 = covarYtel,
                                    piMat = piMat,
                                    pq1Mat = pq1Mat, pq2Mat = pq2Mat,
                                    phi = phi,
                                    subResults = TRUE)
        
        parameters[l, names(expVars)] <- expVars
      }
    }
    
    
    results <- do.call("rbind", results)
    
    if (any((betaInt - betaTel)[-1L] != 0.0))
        MBtype <- "variable"
      else
        MBtype <- "constant"
    
    results <- results %>% 
      add_column(.before = "n", sampling = sampling) %>% 
      add_column(.before = "sampling", N = N) %>% 
      add_column(.after = "n", YtelLaw = factor(YtelLaw)) %>% 
      add_column(.after = "YtelLaw", sdTel = sdTel) %>% 
      add_column(.after = "YtelLaw", YintLaw = factor(YintLaw)) %>% 
      add_column(.after = "YintLaw", sdInt = sdInt) %>% 
      add_column(.after = "sdInt", rho = rho) %>% 
      add_column(.after = "rho",
                 MBtype = factor(MBtype)) %>% 
      add_column(.after = "MBtype", phi = factor(phiType))
    

    suppressMessages(results <- results %>% inner_join(parameters, by = NULL))
    
      
    # We calculate the real variance of the benchmark estimator
    sdTWBenchmark <- var_HT_seq_phi2(expY2 = expYtels, 
                                     covarY2 = covarYtel,
                                     piMat = piMat,
                                     pq1Mat = pq1Mat,
                                     pq2Mat = pq2Mat,
                                     phi = numeric(N),
                                     correcEstimWeights = FALSE) %>% sqrt()
    
    results <- results %>% 
      mutate(expSDTWBenchmark = sdTWBenchmark)
    
    
    results
  }
```

-   `trueYtel`: $t_{tel} =\sum_{k \in U} y_{2k}$ for this iteration
-   `expYtel` : $\mathbb{E}[t_{tel}]=\mathbb{E} [\sum_{k \in U} y_{2k}]$
-   `estPhiBias` : $t_{\phi \hat{\Delta y}}$
-   `estPhiBiasDiffHT` : $\hat{t}_{\hat{p},\phi y_1} - \hat{t}_{\hat{p},\phi y_2}$
-   `truePhiBias` : $t_{\phi\Delta y}=\sum_{k\in U} \phi_k \Delta y_k$
-   `expPhiBias` : $\mathbb{E}[t_{\phi\Delta y}]$
-   `estPhiYintTrueMP` : $\hat{t}_{p,\phi y_1} = \sum_{k \in S_r}\frac{\phi_k y_{1k}}{\pi_k p_{1k}}$
-   `estPhiYintEstMP` : $\hat{t}_{\hat{p},\phi y_1} = \sum_{k \in S_r}\frac{\phi_k y_{1k}}{\pi_k \hat{p}_{1k}}$
-   `estYtelTrueMP` : $\hat{t}_{py_2}:= \hat{t}_{p,\phi y_1} + \hat{t}_{p,\bar{\phi} y_2} - t_{\phi \hat{\Delta y}}$
-   `estYtelEstMP` : $\hat{t}_{\hat{p}y_2}:= \hat{t}_{\hat{p},\phi y_1} + \hat{t}_{\hat{p},\bar{\phi} y_2} - t_{\phi \hat{\Delta y}}$
-   `expVar2` : $\mathbb{V}[\hat{t}_{\hat{p}y_2}]$ PAS ENCORE VRAI, À CONSIDÉRER
-   `TWBenchmark` : $\sum_{k \in S_{mr}} \frac{y_{2k}}{\pi_k(1-p_{1k})p_{2k}}$
-   `benchmark` : $\sum_{k \in S_{mr}} \frac{y_{2k}}{\pi_k(1-\hat{p}_{1k})\hat{p}_{2k}}$
-   `sdTWBenchmark` : standard deviation of the benchmark
-   `calculTotal` : `population` if we calculate $\hat{t}_{\phi \hat{\Delta y}}$ (imputation on the entire population), `sample` if it is $t_{\phi \hat{\Delta y}}$ (on $S_{r\bullet}$)

```{r}
#| echo: false
loop_simulation <- function(parameters, M = 2000L)
{
  nbCores <- detectCores() - 1L
  cluster <- makeCluster(nbCores)
  
  clusterExport(cluster, 
                varlist = "Z",
                envir = environment())

  results <- NULL
  nbResults <- 0L
  
  parameters <- parameters %>% distinct()
  
  betaTelMinus <- betaTel
  betaIntMinus <- betaInt
  
  # Case when the sign a the age coefficient for telephone is positive
  betaTelPlus <- betaTel
  betaTelPlus[-1L] <- -betaTel[-1L]
  betaIntPlus <- betaInt
  betaIntPlus[-1L] <- -betaInt[-1L]
  
  bar <- progress_bar$new(total = nrow(parameters),
                          format = "[:bar] :current/:total (:percent) eta: :eta")
  
  for (i in seq_len(nrow(parameters)))
  {
    tic()
    expParams <- parameters[i, ]
    #print(expParams)
  
    
    if (expParams$signAge == "plus")
    {
      betaIntTemp <- betaIntPlus
      betaTelTemp <- betaTelPlus
    }
    else if (expParams$signAge == "minus")
    {
      betaIntTemp <- betaIntMinus
      betaTelTemp <- betaTelMinus
    }
    
    if (expParams$constBias)
      betaIntTemp[-1L] <- betaTelTemp[-1L]
    
    expResults <- simulationUncounf(N, M = M,
                                    Z = Z,
                                    sampling = expParams$sampling,
                                    alphaInt = alphaInt,
                                    alphaTel = alphaTel,
                                    betaInt = betaIntTemp,
                                    betaTel = betaTelTemp,
                                    YtelLaw = expParams$YtelLaw,
                                    YintLaw = expParams$YintLaw,
                                    phi = expParams$phi,
                                    sdInt = expParams$sdY,
                                    sdTel = expParams$sdY,
                                    seed = 200L,
                                    cluster = cluster)
    
    expResults <- expResults %>% 
      rename(sd = sdInt) %>% 
      select(-sdTel) %>% 
      mutate(signAge = expParams$signAge)
  
    if (i == 1L)
      expNbResults <- nrow(expResults)
    
    nbResults <- nbResults + expNbResults
    expResults <- expResults %>% 
      add_column(.before = 1L, 
                 experiment = factor(rep(nbResults + seq_len(M), 
                                         each = expNbResults / M)))
    
    results <- rbind(results, expResults)
    
    #t <- toc(quiet = TRUE)
    
    #glue("{round(unname(t$toc - t$tic))}s ({i} / {nrow(parameters)})") %>% print()
    bar$tick()
  }
  
  stopCluster(cluster)
  
  results$signAge <- as.factor(results$signAge)
  results$parameter <- as.factor(results$parameter)
  results$probSelect <- as.factor(results$probSelect)
  results$invMatrices <- as.factor(results$invMatrices)
  results$calculTotal <- as.factor(results$calculTotal)
  
  results <- results %>% 
    mutate(.after = "estPhiBias", diffPhiBias = estPhiBias - truePhiBias) %>% 
    mutate(.after = "estPhiBarYtelEstMP", 
           diffYtelEstMP = estYtelEstMP - trueYtel) %>% 
    mutate(.after = "estPhiBarYtelTrueMP",
           diffYtelTrueMP = estYtelTrueMP - trueYtel) %>% 
    mutate(.after = "diffYtelTrueMP", 
           diffWithoutCorrectionEstMP = estPhiYintEstMP + 
             estPhiBarYtelEstMP - 
             trueYtel) %>% 
    mutate(.after = "diffYtelTrueMP", diffBenchmark = benchmark - trueYtel)
  
  results
}
```

# measure effect evaluation

Evaluation of $M$ experiments:

```{r}
M <- 1000L
Kvec <- seq_len(M)
```

```{r}
#| eval: false

load(file = "../results.RData")
```

Here we evaluate our estimators on the simplest cases : Gaussian laws with $\phi_k \equiv \frac{1}{2}$, a few values for $\sigma$ and a changing sign for $\beta_{tel,age}$.

```{r}
#| eval: false
# Grid of all evaluated parameters
parameters <- expand_grid(constBias = c(TRUE, FALSE),
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = "eq",
                          sdY = 1.0) %>%
  # If there is only exponential laws the parameter sdY is of no interest
  mutate(sdY =
           ifelse(YtelLaw == "gaussian" |
                    YintLaw == "gaussian", sdY, "null")) 

resultsGaussian <- loop_simulation(parameters, M = M)
```

# Ex : Gaussian constant measure effect

Results of each parameter for the case of SRS with **constant** measure effect, $\beta_{tel,age} > 0$ and Gaussian laws (with $\sigma =1$).

## measure effect total

$$\text{relError} = \frac{1}{M}\sum_{k=1}^M \frac{\hat{t}_{\phi\Delta y,k}-t_{\phi\Delta y,k}}{\mathbb{E}[t_{\phi\Delta y}]}$$

$$\text{relRMSE} = \frac{\sqrt{\frac{1}{M}\sum_{k=1}^M (\hat{t}_{\phi\Delta y,k}-t_{\phi\Delta y,k})^2}}{|\mathbb{E}[t_{\phi\Delta y}]|}$$

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", 
         MBtype == "constant", signAge == "plus", 
         trueEstimator) %>% 
  select(-sampling, -MBtype, -probSelect) %>% 
  group_by(parameter, invMatrices, calculTotal)

temp %>% 
  summarise(M = n(),
            relErrorWithoutCorr = round(mean(diffWithoutCorrectionEstMP / abs(expYtel)), 4L),
            relError = round(mean(diffPhiBias / abs(expPhiBias)), 4L), 
            relRMSE = round(sqrt(mean(diffPhiBias^2L / expPhiBias^2L)), 4L)) %>% 
  ungroup() %>% 
  arrange(relRMSE)
```

The relative error `relError` is close to zero for each estimator. It is clear that it is an improvement compared to the estimator that totally neglect the bias, i.e. $\hat{t}_{pq\phi1}-\hat{t}_{pq\bar{\phi}2}$. RMSEs are about the same, being in \[0.13, 0.18\]. For the best RMSE we have the model that consider the bias constant (up to a centered noise): the G-COMP model with a simple constant $\delta$ for the MB consideration.

## telephone total estimation

Our different estimators are all approximately unbiased, so we focus on the RMSE appearing when we estimate the total $t_2$ with each estimator:

```{r}
temp %>% 
  summarise(M = n(),
            relEstRMSE = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = mean(expSDTWBenchmark / abs(expYtel))) %>% 
  ungroup() %>% 
  arrange(relEstRMSE)

rm(temp)
```

The `relEstRMSEBenchmark` variable is the estimated RMSE of the benchmark estimator. We can observe that among the different estimators, we have a few of them that offer a **lower RMSE than the benchmark**. We have, with $\phi \equiv \frac{1}{2}$ :

-   G-COMPUTATION with 0, 1 or 2 degrees and a estimation of the total $\phi\Delta$ via the sample or the population

-   The HT Thompson estimator with sample inverse matrices and a total on the sample or the population

Summing the bias on exclusively $S_{r\bullet}$ seems to be effective (with `calculTotal = sample`), which is great because in that case we don't need to know the $z_k$ for units that didn't respond by internet.

The case $\phi = \frac{1}{N}, \frac{2}{N}, \cdots$ gives a RMSE higher than the benchmark RMSE.

With $\phi$ fixed, the other estimators give a RMSE nearly equal the benckmark.

The `relExpRMSETWBenchmark` variable is the expected standard deviation of the benchmark calculated with the true selection probabilities. The bias is equal to zero with the true weights so the RMSE is equal to the standard deviation. Biases of the estimators are nearly zero so RMSE and standard deviations can be compared. We can observe that for every estimator the (estimated) RMSE is **clearly lower** than the standard deviation / RMSE of the benchmark with true probabilities.

## Convergence in law

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", 
         MBtype == "constant", signAge == "plus", 
         phi == "eq", trueEstimator) %>% 
  select(experiment, nInt, nTel, estPhiYintEstMP, expYint, avgEstWeightsInt,
         estPhiYtelEstMP, estPhiBarYtelEstMP, expYtel, normEstPhiBiasDiffHT) %>% 
  group_by(experiment) %>% 
  slice_head(n = 1L) %>% 
  ungroup()
```

### $\hat{t}_{pq\phi1}$

With $\phi \equiv \frac{1}{2}$

```{r}
estimatorsPhi1 <- temp$estPhiYintEstMP

empMean1 <- mean(estimatorsPhi1)

expMean1 <- as.numeric(0.5 * temp[1L, "expYint"])

glue("empirical mean: {empMean1} (expected: {expMean1})")
```

Modifier, utilisation d'un écart-type estimé sur toute la population... :
```{r}

empSD1 <- sd(estimatorsPhi1)

temp %>% 
ggplot() + 
  geom_density(aes(x = estPhiYintEstMP)) + 
  geom_vline(aes(xintercept = empMean1, colour = "constant"), show.legend = TRUE) +
  ggtitle("HT-phi1 estimator with estimated probabilities") +
  xlab("Estimator") +
  scale_color_manual(name = "bias", values = c(constant = "red"))

ks.test(x = (estimatorsPhi1 - empMean1) / empSD1, y = "pnorm")
```

### $\hat{t}_{pq\bar{\phi}2}$

With $\phi \equiv \frac{1}{2}$

```{r}
estimatorsPhi2 <- temp$estPhiYtelEstMP

empMean2 <- mean(estimatorsPhi2)

expMean2 <- 0.5 * as.numeric(temp[1L, "expYtel"])

glue("empirical mean: {empMean2} (expected: {expMean2})")
```

```{r}
empSD2 <- sd(estimatorsPhi2)

temp %>% 
ggplot() + 
  geom_density(aes(x = estimatorsPhi2)) + 
  geom_vline(aes(xintercept = empMean2, colour = "constant"), show.legend = TRUE) +
  ggtitle("HT-phi1 estimator with estimated probabilities") +
  xlab("Estimator") +
  scale_color_manual(name = "bias", values = c(constant = "red"))

ks.test(x = (estimatorsPhi2 - empMean2) / empSD2, y = "pnorm")
```





```{r}
Mbis <- 10000L
```

Simulation with no measure effect:
```{r}
tic()
pv <- profvis::profvis({
resSimTotals <- simulation_tphi(M = Mbis,
                                Z = Z,
                                sampling = "SRS",
                                alphaInt = alphaInt,
                                alphaTel = alphaTel,
                                betaInt = betaTel,
                                betaTel = betaTel,
                                YintLaw = "gaussian",
                                YtelLaw = "gaussian",
                                phi = "eq",
                                sdInt = 1.0,
                                sdTel = 1.0,
                                seed = 200L)
})
glue("{toc()}")
```

## Bias detection

```{r}
simulation_tphi <- function(Z,
                            sampling = "SRS",
                            alphaInt, alphaTel, 
                            betaInt, betaTel, 
                            YintLaw = "gaussian", YtelLaw = "gaussian",
                            sdInt = 1.0, sdTel = 1.0, 
                            rho = 0.8,
                            phi = rep(0.5, N), M = 10000L,
                            n = ceiling(N / 10L),
                            seed = 123L,
                            nCores = detectCores() - 1L)
{
  set.seed(seed)
  
  N <- nrow(Z)
  
  expit <- function(x) 1.0 / (1.0 + exp(-x))

  pInt <- expit(Z %*% alphaInt) %>% as.numeric()
  pq1Mat <- pi_to_pi2(pInt)
  
  pTel <- expit(Z %*% alphaTel) %>% as.numeric()
  pq2Mat <- pi_to_pi2(pTel)

  if (sampling == "SRS")
  {
    pi <- rep(n / N, N)
    piMat <- piMat_SRS(n, N)
  }
  
  
  dataExpY <- gen_expY(Z, 
                       betaInt, YintLaw, sdInt,
                       betaTel, YtelLaw, sdTel)
  
  expYints <- dataExpY$expYints
  sdInts <- dataExpY$sdInts
  
  expYtels <- dataExpY$expYtels
  sdTels <- dataExpY$sdTels
  
  rm(dataExpY)
  
  phi <- set_phi(phi, N)
  
  
  monoSim <- function(...)
  {
    q <- ncol(Z)
        
      
    if (sampling == "SRS")
    {
      selectedSample <- sample(seq_len(N), size = n, replace = FALSE)
      
      I <- logical(N)
      I[selectedSample] <- TRUE
      rm(selectedSample)
    }
    
    
    dataModes <-  gen_choice_bimode(I, pInt, pTel)
    modes <- dataModes$mode
    
    maskInt <- modes == "int"
    nInt <- sum(maskInt)
    maskTel <- modes == "tel"
    nTel <- sum(maskTel)
    
    trueProbsSelect <- dataModes$probSelect
    
    rm(dataModes)
    
    trueWeights <- (pi * trueProbsSelect)^-1L
  
    # Estimation of mode selection probabilities
    estimProbsData <-
      MMsampling::estim_response_prob_sequential(I, pi, Z,
                                                 modes,
                                                 c("int", "tel"),
                                                 chosenOnly = FALSE)

    estimProbsSelect <-
       MMsampling::get_value_by_mode(estimProbsData$unconditional, modes)
    estimWeights <- (pi * estimProbsSelect)^-1L
    
    # Generating values for Y_int and Y_tel
    dataY <- gen_Y(Z, 
                   betaInt, sdInt, YintLaw,
                   betaTel, sdTel, YtelLaw,
                   expYints = expYints, expYtels = expYtels,
                   rho = rho)
    
    Yints <- dataY$Yint
    Ytels <- dataY$Ytel
    
    rm(dataY)
    
    # Creating the observed outcome vector
    Yobs <- set_Yobs(Yint = Yints, Ytel = Ytels, modes = modes)
    
    # Estimations with Y1
      # Estimation of the internet design covariance matrix
    estimpq1 <- estimProbsData$conditional[, "int"]
    # estimpq1Mat <- pi_to_pi2(estimpq1)
    estimpq1MatSr <- pi_to_pi2(estimpq1[maskInt])
    estimpq1MatSmr <- pi_to_pi2(estimpq1[maskTel])
    
     # Total of phi_k y_1k with true probabilities
    expPhi1 <- sum(phi * expYints)
    totPhi1TW <- sum(phi[maskInt] * trueWeights[maskInt] * Yobs[maskInt])
    
        # Estimation of the approximate variance of the total estimator
        # the variance of Y1 is assumed known and homoscedastic for simplicity
    # apprVarPhi1TW <- 
    #   MMsampling::estim_appr_var_seq_phi1(Yobs,
    #                                       modes, 
    #                                       I,
    #                                       piMat,
    #                                       pq1Mat,
    #                                       Z,
    #                                       phi, 
    #                                       sd1 = sdInt,
    #                                       correcEstimWeights = FALSE,
    #                                       independenceq1 = TRUE)
    
    apprVarPhi1TW <- NA_real_
    
      # Total of phi_k y_1k with estimated probabilities
    totPhi1EW <- sum(phi[maskInt] * estimWeights[maskInt] * Yobs[maskInt])
    # totPhi1EW <- NA_real_
    
    apprVarPhi1EW <-
      MMsampling::estim_appr_var_seq_phi1(Yobs,
                                          modes,
                                          I,
                                          piMat,
                                          estimpq1MatSr,
                                          Z,
                                          phi,
                                          sd1 = sdInt,
                                          correcEstimWeights = TRUE,
                                          independenceq1 = TRUE,
                                          p1 = estimpq1,
                                          pq1MatSr = estimpq1MatSr)
     
    # apprVarPhi1EW <- NA_real_
      # parameter equal to
      #  phi_k y_1k / sum of the phi_k on U
    expMeanPhi1 <- expPhi1 / sum(phi)
    meanPhi1TW <- 
      sum(phi[maskInt] * trueWeights[maskInt] * Yobs[maskInt]) /
      sum(phi[maskInt] * trueWeights[maskInt])
    
    # apprVarMeanPhi1TW <- 
    #   MMsampling::estim_var_mean_phi1(Yobs,
    #                                   modes, 
    #                                   I,
    #                                   piMat,
    #                                   pq1Mat,
    #                                   Z,
    #                                   phi, 
    #                                   correcEstimWeights = FALSE)
    # 
    
    apprVarMeanPhi1TW <- NA_real_ 
    
    meanPhi1EW <-
      sum(phi[maskInt] *
            (estimWeights[maskInt] * Yobs[maskInt] - mean(Yints))) /
      sum(phi[maskInt] * estimWeights[maskInt])
    # meanPhi1EW <- NA_real_
    
    apprVarMeanPhi1EW <-
      MMsampling::estim_var_mean_phi1(Yobs,
                                      modes,
                                      I,
                                      piMat,
                                      estimpq1MatSr,
                                      Z,
                                      phi,
                                      correcEstimWeights = TRUE,
                                      independenceq1 = TRUE,
                                      p1 = estimpq1,
                                      pq1MatSr = estimpq1MatSr)
    
    # apprVarMeanPhi1EW <- NA_real_
    
    # Estimations with Y2
    
      # Estimation of the internet design covariance matrix
    estimpq2 <-  estimProbsData$conditional[, "tel"]
    # estimpq2Mat <- pi_to_pi2(estimpq2)
    estimpq2MatSmr <- pi_to_pi2(estimpq2[maskTel])

    
      # Total of phi_k y_2k with true probabilities
    totPhi2TW <- sum(phi[maskTel] * trueWeights[maskTel] * Yobs[maskTel])
    
    
        # Estimation of the approximate variance of the total estimator
        # the variance of Y2 is assumed known and homoscedastic for simplicity
    # apprVarPhi2TW <-
    #   MMsampling::estim_appr_var_seq_phi2(Yobs,
    #                                       modes,
    #                                       I,
    #                                       piMat,
    #                                       pq1Mat,
    #                                       pq2Mat,
    #                                       Z,
    #                                       phi,
    #                                       sd2 = sdTel,
    #                                       correcEstimWeights = FALSE,
    #                                       independenceq1 = TRUE,
    #                                       independenceq2 = TRUE)

    apprVarPhi2TW <- NA_real_
    
      # Total of phi_k y_2k with estimated probabilities
    expPhi2 <- sum(phi * expYtels)
    totPhi2EW <- sum(phi[maskTel] * estimWeights[maskTel] * Yobs[maskTel])
    # totPhi2EW <- NA_real_
    
    
    apprVarPhi2EW <-
      MMsampling::estim_appr_var_seq_phi2(Yobs,
                                          modes,
                                          I,
                                          piMat,
                                          estimpq1MatSmr, # pq1Mat,
                                          estimpq2MatSmr, #pq2Mat,
                                          Z,
                                          phi,
                                          sd2 = sdTel,
                                          correcEstimWeights = TRUE,
                                          independenceq1 = TRUE,
                                          independenceq2 = TRUE,
                                          p1 = estimpq1,
                                          p2 = estimpq2,
                                          pq1MatSmr = estimpq1MatSmr,
                                          pq2MatSmr = estimpq2MatSmr)
    
    # apprVarPhi2EW <- NA_real_
    
      # parameter equal to
      #  phi_k y_2k / sum of the phi_k on U,
    expMeanPhi2 <- expPhi2 / sum(phi)
    meanPhi2TW <- 
      sum(phi[maskTel] * trueWeights[maskTel] * Yobs[maskTel]) /
      sum(phi[maskTel] * trueWeights[maskTel])
    
    # apprVarMeanPhi2TW <-
    #   MMsampling::estim_var_mean_phi2(Yobs,
    #                       modes,
    #                       I,
    #                       piMat,
    #                       pq1Mat,
    #                       pq2Mat,
    #                       Z,
    #                       phi,
    #                       correcEstimWeights = FALSE)
    
    apprVarMeanPhi2TW <- NA_real_
    
    meanPhi2EW <-
      sum(phi[maskTel] *
            (estimWeights[maskTel] * Yobs[maskTel] - mean(Ytels))) /
      sum(phi[maskTel] * estimWeights[maskTel])
    
    # meanPhi2EW <- NA_real_
    
    apprVarMeanPhi2EW <-
      MMsampling::estim_var_mean_phi2(Yobs,
                                      modes,
                                      I,
                                      piMat,
                                      estimpq1MatSmr,
                                      estimpq2MatSmr,
                                      Z,
                                      phi,
                                      correcEstimWeights = TRUE,
                                      independenceq1 = TRUE,
                                      independenceq2 = TRUE,
                                      p1 = estimpq1,
                                      p2 = estimpq2,
                                      pq1MatSmr = estimpq1MatSmr,
                                      pq2MatSmr = estimpq2MatSmr)
    
    # apprVarMeanPhi2EW <- NA_real_
    
    # rm(estimpq1Mat, estimpq2Mat)
    
    tibble(estimand = rep(c("phi1", "mean phi1", 
                            "phi2", "mean phi2"), each = 2L), 
           expEstimand = rep(c(expPhi1, expMeanPhi1, expPhi2, expMeanPhi2), 
                             each = 2L),
           estimator = c("totPhi1TW", "totPhi1EW", 
                         "meanPhi1TW", "meanPhi1EW", 
                         "totPhi2TW", "totPhi2EW",
                         "meanPhi2TW", "meanPhi2EW"), 
           estimation = c(totPhi1TW, totPhi1EW, 
                          meanPhi1TW, meanPhi1EW, 
                          totPhi2TW, totPhi2EW,
                          meanPhi2TW, meanPhi2EW), 
           apprVar = c(apprVarPhi1TW, apprVarPhi1EW, 
                       apprVarMeanPhi1TW, apprVarMeanPhi1EW,
                       apprVarPhi2TW, apprVarPhi2EW,
                       apprVarMeanPhi2TW, apprVarMeanPhi2EW)) %>% 
      mutate(weights = rep_len(c("true", "estimated"), length.out = n()),
             .after = "estimator")
  }
  
  pv <- profvis::profvis(lapply(seq_len(M), FUN = monoSim))
  # return(profvis::profvis(lapply(seq_len(10L), FUN = monoSim))) #prof_output = "test.res"))
  # pv <- profvis::profvis(lapply(seq_len(10L), FUN = monoSim))
  # pv <- microbenchmark::microbenchmark(monoSim(), times = 10L)
  return(pv)
  
  # res <- lapply(seq_len(M), FUN = monoSim)
  # res <- mclapply(seq_len(M), FUN = monoSim, mc.cores = nCores)
  do.call("rbind", res)
}
```

We would like to test our ability to detect the presence or absence of a constant measure effect with the superposition of confidence intervals of $\hat{t}_{pq\phi1}$ and $\hat{t}_{pq\phi2}$. If there is no measure effect then we can assume there will be superposition of the confidence intervals.

We will change the value of the constant measure effect $\delta$ and test the coverage of our test. We consider the ratio between the measure bias and the expected mean of the telephone answers.

```{r}
ratioDeltas <- c(0.0, 0.1, 0.25, 0.5, 0.75, 1.0)
expMean2 <- mean(Z %*% betaTel)
deltas <- ratioDeltas * abs(expMean2)

deltas
```

Then for each measure bias we try the recognition of an eventual measure bias, with confidence intervals with errors $\alpha_1 = \alpha_2 = 0.05$.

For centered it is not really an estimation because it uses the empiracal mean on U


```{r}
ratiosn <- c(0.16, 0.25, 0.4)
nvec <- ratiosn * N
nMin <- min(nvec)
sigmavec <- c(1.0, 2.0, 5.0)
ratiosigmavec <- c(0.25, 0.5, 1.0, 2.0, 4.0)
rhovec <- c(0.8, 0.9)
rhoRef <- 0.8

paramVarn <- 
  expand.grid(ratio_delta = ratioDeltas, 
              ratio_n = ratiosn, 
              sigma = sigmavec[1L], rho = rhoRef)
paramVarRho <- 
  expand.grid(ratio_delta = ratioDeltas, ratio_n = nMin, sigma = sigmavec[1L], 
              rho = rhovec)
paramVarSigma <- 
  expand.grid(ratio_delta = ratioDeltas, ratio_n = nMin,
              sigma = sigmavec[-1L], 
              rho = rhoRef)

paramVarRatioSigma <- expand.grid(ratio_delta = ratioDeltas, ratio_n = nMin,
                                  sigma1 = sigmavec[1L],
                                  sigma2 = sigmavec[1L] * ratiosigmavec,
                                  rho = rhoRef)

parameters <- 
  rbind(paramVarn, paramVarRho, paramVarSigma) %>% 
  mutate(sigma1 = sigma, sigma2 = sigma) %>% 
  select(-sigma) %>% 
  rbind(paramVarRatioSigma) %>% 
  filter(sigma1 == sigmavec[1L] | !ratio_delta %in% c(0.1, 0.75)) %>% 
  mutate(delta = ratio_delta * abs(expMean2), .after = "ratio_delta") %>% 
  mutate(n = ratio_n * N, .after = "ratio_n") %>% 
  distinct()

rm(paramVarn, paramVarRho, paramVarSigma, paramVarRatioSigma)

summary(parameters)
parameters
```
```{r}
Msim <- 10000L
```

```{r}
resSimBiasDetection <- NULL

pb <- progress::progress_bar$new(format = "[:bar] :percent eta: :eta",
                                 total = nrow(parameters))

set.seed(200L)
for (k in seq_len(nrow(parameters)))
{
  pb$tick(0L)

  delta <- parameters[k, "delta"]
  betaIntTemp <- betaTel
  betaIntTemp[1L] <- betaIntTemp[1L] + delta
  
  
  resTemp <- simulation_tphi(M = Msim,
                             Z = Z,
                             sampling = "SRS",
                             alphaInt = alphaInt,
                             alphaTel = alphaTel,
                             betaInt = betaIntTemp,
                             betaTel = betaTel,
                             YintLaw = "gaussian",
                             YtelLaw = "gaussian",
                             phi = "eq",
                             sdInt = parameters[k, "sigma1"],
                             sdTel = parameters[k, "sigma2"],
                             n = parameters[k, "n"],
                             rho = parameters[k, "rho"],
                             seed = NULL,
                             nCores = 20L)
  
  break
  resSimBiasDetection <-
    cbind(parameters[k, ], resTemp) %>%
    rbind(resSimBiasDetection)

  pb$tick()
  seed <- .Random.seed
}

resTemp
# rm(resTemp)
# 
# resSimBiasDetection <- 
#   resSimBiasDetection %>% 
#   arrange(n, sigma, ratio_delta)
```

```{r}
rm(seed)
```


### Variance estimators for the totals

We use the results to test the validity of our approximate variance estimators, that will be use for the bias detection.

Here are the true variance of $\hat{t}_{e\phi1}$, in the case of independency for both mode counterfactuals (intra & inter)
```{r}
true_variance_tephi1 <- function(Y1exp, pi, covpMat, p1, phi, sd1 = 1.0)
{
  Y1expMat <- Y1exp %*% t(Y1exp)
  diag(Y1expMat) <- sd1^2L + Y1exp^2L
  
  # q1 variability
  sum((1.0 - p1) * phi^2L / (pi * p1) * (sd1^2L + Y1exp^2L)) +
  # S variability
    t(phi / pi) %*% (covpMat * Y1expMat) %*% (phi / pi) +
  # Y1 variability
    sum(phi^2L) * sd1^2L
}
```

And for $\hat{t}_{e\phi2}$ :
```{r}
true_variance_tephi2 <- function(Y2exp, pi, covpMat, p1, p2, phi, sd2 = 1.0)
{
  p1Bar <- 1.0 - p1
  Y2expMat <- Y2exp %*% t(Y2exp)
  diag(Y2expMat) <- sd2^2L + Y2exp^2L
  # q2 variability
    sum((1.0 - p2) * phi^2L / (pi * p1Bar * p2) * (sd2^2L + Y2exp^2L)) +
  # q1 variability
    sum(p1 * phi^2L / (pi * p1Bar) * (sd2^2L + Y2exp^2L)) +
  # S variability
    t(phi / pi) %*% (covpMat * Y2expMat) %*% (phi / pi) +
  # Y2 variability
    sum(phi^2L) * sd2^2L
}
```

Those values can be used as comparison. 


Here we show average approximate variance and variance of different estimators, with no measure effect.

NOTE : Calculation is made on the entire set of iterations, whereas in some next parts we will use only a half of them.

```{r}
temp <- resSimBiasDetection %>% 
  filter(ratio_delta == 0.0)

temp <- temp %>% 
  # slice_head(n = nrow(temp) / 2L) %>% 
  group_by(estimand, weights, n, sigma, rho) %>% 
  summarize(M = n(),
            expEstimand = mean(expEstimand),
            mean = mean(estimation), 
            avg_AV = mean(apprVar),
            var = var(estimation)) %>% 
  mutate(ratioVar = avg_AV / var)

temp
```

The following table is restricted to the estimators $\hat{t}_{pq\phi1}$ and $\hat{t}_{pq\phi2}$. We have a new column that indicates the true variance of $\hat{t}_{e\phi1}$ and $\hat{t}_{e\phi2}$.


```{r}
temp <- 
resSimBiasDetection %>% 
  # filter(ratio_delta == 0.0) %>% 
  filter(weights == "estimated", estimand %in% c("phi1", "phi2")) %>% 
  select(-weights)


expYtels <- Z %*% betaTel

# List of the expected values of the web counterfactuals for each delta
expYints_list <- list("0" = expYtels)
for (delta in unique(temp$delta))
  expYints_list[[as.character(delta)]] <- expYtels + delta

covarSRS <- covar_SRS(n, N)

# List of the variance of t_ephi2 for each value of sigma
vartotphi2TW_list <- list()
for (sigma in unique(temp$sigma))
{
  vartotphi2TW_list[[as.character(sigma)]] <- 
    true_variance_tephi2(expYtels,
                         pi, covarSRS, 
                         pInt, pTel, 
                         phi, 
                         sd2 = sigma)
}

varsTW <- numeric(nrow(temp))

for (k in seq_len(nrow(temp) / 2L))
{
  delta <- temp[2L * k, "delta"]
  sigma <- temp[2L * k, "sigma"]
  # expData <- gen_expY(Z, 
  #                     betaInt = betaInt, sdInt = sigma, 
  #                     betaTel = betaTel, sdTel = sigma)

  n <- temp[k, "n"]
  vartotphi1TW <- true_variance_tephi1(expYints_list[[as.character(delta)]], 
                                       pi, covarSRS, 
                                       pInt,
                                       phi, 
                                       sd1 = sigma)

  vartotphi2TW <- vartotphi2TW_list[[as.character(sigma)]]

  varsTW[c(2L * k - 1L, 2L * k)] <- c(vartotphi1TW, vartotphi2TW)
}


rm(expData)


temp %>% 
  add_column(varTW = varsTW)
  # conversion_latex(nom = "simulation_estim_var_approx", caption = "Variance estimation")

rm(delta, sigma, n, temp, vartotphi1TW, vartotphi2TW, varsTW)
```


### Superposition

`count_superposition` gives a few results on the different iterations and the estimators:

-   mean of the estimator (for web and telephone)

-   standard deviation of the estimator based on the first half iterations (for web and telephone)

-   average of the approximate variance estimator (for web and telephone)

-   \% of confidence intervals overlapped with unbiased variance estimator for the second half part of the iterations, based on the previous estimation of the standard deviations (`percSuperpositionsUV`)

-   \% of confidence intervals overlapped with approximate variance estimators (`percSuperpositionsAV`)

-   \% of situations when at least one confidence interval contains the other total estimator for the second half part of the iterations, based on the previous estimation of the standard deviations (`percIntersectionsUV`)

-   \% of situations when at least one confidence interval contains the other total estimator with approximate variance estimators (`percIntersectionsAV`)

If `correctAlpha` is true then the size of the intervals are changed in the order to have a first type error equal to $\alpha$ (that would be exact if the totals vector is gaussian and there would have no covariance between the totals and if the used variance estimators were the true variances). See [Comparing the overlapping of two independent confidence intervals with a single confidence interval for two normal population parameters](https://doi.org/10.1016/j.jspi.2010.04.057).

The "intersection" method is supposed to be anti-conservative.


```{r}
count_superposition <- function(data1, data2, alpha = 0.05, correctAlpha = TRUE)
{
  ratio_delta <- data1$ratio_delta[1L]
  delta <- data1$delta[1L]
  sigma <- data1$sigma[1L]
  n <- data1$n[1L]
  
  M <- nrow(data1)
  
  # Check with "true" variance 
  # (see if with the use of "true variance" (Monte Carlo estimation)
  # we have good results.
  # The first half will be used to estimate the variance. The second
  # half to test the superpositions
  # We only keep the second half of the results for estimations that will
  # use sd1 or sd2 in one way or another
  subsetCalcVar <- seq_len(M / 2L)
  
    # Estimated standard deviation of the two results
    # based on the first half results
  sd1 <- sd(data1$estimation[subsetCalcVar])
  sd2 <- sd(data2$estimation[subsetCalcVar])
  
  if (correctAlpha)
  {
    KUV <- sd1 / sd2
    gammaUV <- 2.0 * pnorm(qnorm(alpha / 2.0) * sqrt(1.0 + KUV^2L) / (1.0 + KUV))
    rm(KUV)
  }
  else
    gammaUV <- alpha
  
  
  mean1 <- mean(data1$estimation[subsetCalcVar])
  mean2 <- mean(data2$estimation[subsetCalcVar])
  
  # Test with approximate variance estimators
    # average of the approximate variance estimators
    # (with the first half for comparison)
  AV1 <- mean(data1$apprVar[subsetCalcVar])
  AV2 <- mean(data2$apprVar[subsetCalcVar])
  
    # Confidence intervals with unbiased estimator for the variance
    #   first mode
  CI1lowerUV <- data1$estimation[-subsetCalcVar] - 
    qnorm(1.0 - gammaUV / 2.0) * sd1
  CI1upperUV <- data1$estimation[-subsetCalcVar] + 
    qnorm(1.0 - gammaUV / 2.0) * sd1
  
    #   second mode
  CI2lowerUV <- data2$estimation[-subsetCalcVar] - 
    qnorm(1.0 - gammaUV / 2.0) * sd2
  CI2upperUV <- data2$estimation[-subsetCalcVar] + 
    qnorm(1.0 - gammaUV / 2.0) * sd2
  
    #   Check if there is a superposition of the two confidence intervals
  superpositionUV <- CI1lowerUV <= CI2upperUV & CI2lowerUV <= CI1upperUV
  percSuperpositionsUV <- 100.0 * mean(superpositionUV)
  
    #   Check if there is a superposition that contains both
    #   estimators
  intersectionUV <- 
    (CI1lowerUV <= data2$estimation[-subsetCalcVar] & 
    data2$estimation[-subsetCalcVar] <= CI1upperUV) |
    (CI2lowerUV <= data1$estimation[-subsetCalcVar] & 
    data1$estimation[-subsetCalcVar] <= CI2upperUV)
  percIntersectionsUV <- 100.0 * mean(intersectionUV)
  
    # Confidence intervals with approximate variance of each iteration
  if (correctAlpha)
  {
    KAV <- sqrt(AV1 / AV2)
    gammasAV <-
      2.0 * pnorm(qnorm(alpha / 2.0) * sqrt(1.0 + KAV^2L) / (1.0 + KAV))
    rm(KAV)
  }
  else
    gammasAV <- alpha
  
  
    # , for all iterations
    #   first mode
  CI1lowerAV <- data1$estimation - 
    qnorm(1.0 - gammasAV / 2.0) * sqrt(data1$apprVar)
  CI1upperAV <- data1$estimation + 
    qnorm(1.0 - gammasAV / 2.0) * sqrt(data1$apprVar)
  
    #   second mode
  CI2lowerAV <- data2$estimation - 
    qnorm(1.0 - gammasAV / 2.0) * sqrt(data2$apprVar)
  CI2upperAV <- data2$estimation + 
    qnorm(1.0 - gammasAV / 2.0) * sqrt(data2$apprVar)
  
    #   Check if there is a superposition of the two confidence intervals
  superpositionAV <- CI1lowerAV <= CI2upperAV & CI2lowerAV <= CI1upperAV
  percSuperpositionsAV <- 100.0 * mean(superpositionAV)
  
    #   Check if there is a superposition that contains both
    #   estimators
  intersectionAV <- 
    (CI1lowerAV <= data2$estimation & data2$estimation <= CI1upperAV) |
    (CI2lowerAV <= data1$estimation & data1$estimation <= CI2upperAV)
  percIntersectionsAV <- 100.0 * mean(intersectionAV)
  
  
  tibble(ratio_delta = ratio_delta,
         sigma = sigma,
         delta = delta,
         n = n,
         correctAlpha = ifelse(correctAlpha, "corrected", "unchanged"),
         Mvar = M - length(subsetCalcVar),
         MAV = M,
         mean1 = mean1,
         AV1 = AV1, var1 = sd1^2L,
         mean2 = mean2,
         AV2 = AV2, var2 = sd2^2L,
         gammaUV = gammaUV,
         percSuperpositionsUV = percSuperpositionsUV,
         percIntersectionsUV = percIntersectionsUV,
         gammaAV = gammasAV,
         percSuperpositionsAV = percSuperpositionsAV,
         percIntersectionsAV = percIntersectionsAV)
}
```

#### With the outcomes

```{r}
absResTW <- NULL
for (delt in deltas)
{
  for (sigm in sigmavec)
  {
    for (nb in nvec)
    {
      data1 <- resSimBiasDetection %>% filter(delta == delt,
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "phi1", 
                                              weights == "true")
      
      data2 <- resSimBiasDetection %>% filter(delta == delt, 
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "phi2", 
                                              weights == "true")
    
      for (correctAlpha in c(FALSE, TRUE))
      {
        if (nrow(data1) != 0L)
          absResTW <- 
              rbind(absResTW, count_superposition(data1, data2, 
                                                  correctAlpha = correctAlpha))
      }
    }
    
    
  }
}

rm(data1, data2, delt, sigm)

absResTW <- as_tibble(absResTW) %>% 
  mutate(weights = "true", .before = 1L) %>% 
  mutate(ratioVar1 = AV1 / var1, .after = "var1") %>% 
  mutate(ratioVar2 = AV2 / var2, .after = "var2")
```

```{r}
absResEW <- NULL
for (delt in deltas)
{
  for (sigm in sigmavec)
  {
    for (nb in nvec)
    {
      data1 <- resSimBiasDetection %>% filter(delta == delt,
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "phi1", 
                                              weights == "estimated")
  
      data2 <- resSimBiasDetection %>% filter(delta == delt, 
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "phi2", 
                                              weights == "estimated")
      
      for (correctAlpha in c(FALSE, TRUE))
      {
        if (nrow(data1) != 0L)
          absResEW <- 
            rbind(absResEW, count_superposition(data1, data2,
                                                correctAlpha = correctAlpha))
      }
    }
    
  }
}

rm(data1, data2, delt, sigm)

absResEW <- as_tibble(absResEW) %>% 
  mutate(weights = "estimated", .before = 1L) %>% 
  mutate(ratioVar1 = AV1 / var1, .after = "var1") %>% 
  mutate(ratioVar2 = AV2 / var2, .after = "var2")

absRes <- rbind(absResTW, absResEW)

rm(absResTW, absResEW)
```

```{r}
absRes %>% 
  group_by(weights, n, correctAlpha, sigma, ratio_delta) %>% 
  arrange(weights, n, sigma, ratio_delta, correctAlpha)
```


Superposition on the true total is not great. In fact we have that

$$\max \{\alpha_1,\alpha_2\}\leq\alpha_{12}\leq\alpha_1 + \min\{\alpha_1,\alpha_2\} + \alpha_2$$ with $1-\alpha_{12}$ the probability of having a superposition on the supposed total $t_{\phi2}$ of the two confidence intervals.

Using the superposition implies a lot of false negative non-measure bias detection. The percentage of superposition becomes zero only after a ratio of $1.0$.

```{r}
title <- glue("Superposition - Impact of the coverage rate correction (sigma = 1, n = {nMin})")
absRes %>% 
  mutate(sigma = as.factor(sigma)) %>% 
  filter(weights == "estimated", sigma == 1.0, n == nMin) %>% 
  ggplot() +
  geom_point(aes(x = ratio_delta,
                 y = percSuperpositionsAV,
                 colour = correctAlpha)) +
  # geom_smooth(aes(x = ratio_delta,
  #                y = percSuperpositionsAV, colour = sigma, n = 3)) +
  ggtitle(title)

rm(title)
```



```{r}
title <- glue("Percentage of superposition with true probabilities (n = {nMin})")
absRes %>% 
  mutate(sigma = as.factor(sigma)) %>% 
  filter(weights == "true", correctAlpha == "corrected", n == nMin) %>% 
  ggplot() +
  geom_point(aes(x = ratio_delta,
                 y = percSuperpositionsAV,
                 colour = sigma)) +
  # geom_smooth(aes(x = ratio_delta,
  #                y = percSuperpositionsAV, colour = sigma, n = 3)) +
  ggtitle(title)

rm(title)
```

```{r}
title <- glue("Percentage of superposition with estimated probabilities (n = {nMin})")
absRes %>% 
  mutate(sigma = as.factor(sigma)) %>% 
  filter(weights == "estimated", correctAlpha == "corrected", n == nMin) %>% 
  ggplot() +
  geom_point(aes(x = ratio_delta,
                 y = percSuperpositionsAV,
                 colour = sigma)) +
  # geom_smooth(aes(x = ratio_delta,
  #                y = percSuperpositionsAV, colour = sigma, n = 3)) +
  ggtitle(title)
rm(title)
```


#### With the oucome means

```{r}
centeredResTW <- NULL
for (delt in deltas)
{
  for (sigm in sigmavec)
  {
    for (nb in nvec)
    {
      data1 <- resSimBiasDetection %>% filter(delta == delt,
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "mean phi1", 
                                              weights == "true")
  
      data2 <- resSimBiasDetection %>% filter(delta == delt, 
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "mean phi2", 
                                              weights == "true")
      
      for (correctAlpha in c(FALSE, TRUE))
      {
        if (nrow(data1) != 0L)
          centeredResTW <- 
              rbind(centeredResTW, count_superposition(data1, data2, 
                                                  correctAlpha = correctAlpha))
      }
    }
    
  }
}

rm(data1, data2, delt)

centeredResTW <- as_tibble(centeredResTW) %>% 
  mutate(weights = "true", .before = 1L) %>% 
  mutate(ratioVar1 = AV1 / var1, .after = "var1") %>% 
  mutate(ratioVar2 = AV2 / var2, .after = "var2")
```

```{r}
centeredResEW <- NULL
for (delt in deltas)
{
  for (sigm in sigmavec)
  {
    for (nb in nvec)
    {
      data1 <- resSimBiasDetection %>% filter(delta == delt,
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "mean phi1", 
                                              weights == "estimated")
  
      data2 <- resSimBiasDetection %>% filter(delta == delt, 
                                              sigma == sigm,
                                              n == nb,
                                              estimand == "mean phi2", 
                                              weights == "estimated")
      
      for (correctAlpha in c(FALSE, TRUE))
      {
        if (nrow(data1) != 0L)
          centeredResEW <- 
              rbind(centeredResEW, count_superposition(data1, data2, 
                                                  correctAlpha = correctAlpha))
      }
      
    }
    
  }
}

rm(data1, data2, delt)

centeredResEW <- as_tibble(centeredResEW) %>% 
  mutate(weights = "estimated", .before = 1L) %>% 
  mutate(ratioVar1 = AV1 / var1, .after = "var1") %>% 
  mutate(ratioVar2 = AV2 / var2, .after = "var2")

centeredRes <- rbind(centeredResTW, centeredResEW)

rm(centeredResTW, centeredResEW)
```

```{r}
centeredRes %>% 
  group_by(weights, correctAlpha, sigma, ratio_delta) %>% 
  arrange(weights, sigma, ratio_delta, correctAlpha)
```


The columns with with respect to the web outcomes are constant because the `seed` is the same for each `ratio_delta` and adding a constant measure effect does not impact the value of the estimator.


```{r}
title <- glue("Percentage of superposition with mean and true probabilities (n = {nMin})")
centeredRes %>% 
  mutate(sigma = as.factor(sigma)) %>% 
  filter(weights == "true", correctAlpha == "corrected", n == nMin) %>% 
  ggplot() +
  geom_point(aes(x = ratio_delta,
                 y = percSuperpositionsAV,
                 colour = sigma)) +
  # geom_smooth(aes(x = ratio_delta,
  #                y = percSuperpositionsAV, colour = sigma, n = 3)) +
  ggtitle(title)
rm(title)
```

```{r}
title <- glue("Percentage of superposition with mean and estimated probabilities (n = {nMin})")

centeredRes %>% 
  mutate(sigma = as.factor(sigma)) %>% 
  filter(weights == "estimated", correctAlpha == "corrected", n == nMin) %>% 
  ggplot() +
  geom_point(aes(x = ratio_delta,
                 y = percSuperpositionsAV,
                 colour = sigma)) +
  # geom_smooth(aes(x = ratio_delta,
  #                y = percSuperpositionsAV, colour = sigma, n = 3)) +
  ggtitle(title)

rm(title)
```

### HT-difference

We would like to check if, in the case of a constant bias, the estimator

$$\frac{1}{||\phi||_1}(\hat{t}_{pq\phi1}-\hat{t}_{pq\phi2})$$

detects the bias, in the sense of its expected value is equal to the constant bias $\delta$. Moreover, we would like to be sure that this estimator tends to a normal law. Of course this estimator shouldn't be used to estimate the total of the bias in the global estimator, because it would directly erase the information coming from $S_{r\bullet}$.

Here we use the empirical standard deviation based on our $M$ observations.

#### $\delta = 1$

For $\delta = 1$, we would like to have an empirical mean close to it:

```{r}
estimBias <- temp$normEstPhiBiasDiffHT
empMean <- mean(estimBias)
glue("empirical mean: {empMean} (expected: {1})")
```

Note that both the estimators used are only approximately unbiased.

we check the normality of our estimator:

```{r}
empSD <- sd(estimBias)
temp %>% 
ggplot() + 
  geom_density(aes(x = normEstPhiBiasDiffHT)) + 
  geom_vline(aes(xintercept = 1.0, colour = "constant"), show.legend = TRUE) +
  ggtitle("Detection of a constant bias") +
  xlab("normalized HT-difference estimator") +
  scale_color_manual(name = "bias", values = c(constant = "red"))

ks.test(x = (estimBias - 1.0) / empSD, y = "pnorm")
```

With an important p-value the Kolmogorov-Smirnov test ensure that our estimator is normal.

#### Coverage depending on $\delta$

Then we evaluate the coverage of our confidence intervals. For $\alpha = 0.05$, we have

```{r}
alpha <- .05

nInt <- as.numeric(temp$nInt)

resDetection <- data.frame(ratio = ratioDeltas, 
                           delta = deltas, 
                           percPresenceZero = 0.0,
                           percPresenceDelta = 0.0)


for (delta in deltas)
{
  estimatorPhiDelta <- 1.0 / (0.5 * N) * (estimatorsPhi1 + 
  .5 * avgEstWeightsInt * nInt * (delta - 1.0) - 
  estimatorsPhi2)

  empSDPhiDelta <- sd(estimatorPhiDelta)

  CIlower <-  estimatorPhiDelta - qnorm(1.0 - alpha / 2.0) * empSDPhiDelta
  CIupper <- estimatorPhiDelta + qnorm(1.0 - alpha / 2.0) * empSDPhiDelta
  
  presenceZero <- CIlower <= 0.0 & 0.0 <= CIupper
  
  percPresenceZero <- 100.0 * mean(presenceZero) 
  
  presenceDelta <- CIlower <= delta & delta <= CIupper
  
  percPresenceDelta <- 100.0 * mean(presenceDelta)
  
  resDetection[resSuperpositions$delta == delta, 
                    c("percPresenceZero", "percPresenceDelta")] <- 
    c(percPresenceZero, percPresenceDelta)
}


resDetection
```

```{r}
empSD <- sd(estimBias)

alpha <- 0.05
CIlower <- estimBias - qnorm(1.0 - alpha / 2.0) * empSD
CIupper <- estimBias + qnorm(1.0 - alpha / 2.0) * empSD

coverage <- mean(CIlower <= 1.0 & 1.0 <= CIupper)

glue("coverage: {coverage} (expected: {1 - alpha})")
```

```{r}
rm(deltas, ratioDeltas)
```

# Ex : Gaussian variable measure effect

Results of each parameter for the case of SRS with **variable** measure effect and Gaussian laws and $\sigma = 1$. First we focus on the bias:

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", MBtype == "variable", trueEstimator) %>% 
  select(-sampling, -MBtype, -probSelect) %>% 
  group_by(sd, signAge, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(signAge) %>% 
  arrange(abs(relError), .by_group = TRUE) %>% 
  ungroup()

temp %>% 
  select(-relRMSEParam, -relEstRMSEBenchmark, -relExpRMSETWBenchmark)
```

We can observe that the relative estimated bias is close to zero.

Even with a variable measure effect, the G-COMP model with only a constant for the bias is approximately unbiased. It is due to the fact that we chose here a centered measure effect, respectively to the couple (age, sex).

And now the RMSE:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  mutate(percRMSE = 100.0 * relRMSEParam / relEstRMSEBenchmark, .after = relEstRMSEBenchmark) %>% 
  arrange(relRMSEParam, .by_group = TRUE)
```

The estimator HT with **complete** inverse matrices and summing the bias on the sample only is the greatest (in term of RMSE). Its RMSE is better than the RMSE of the benchmark, which is itself better than the benchmark with true probabilities.

Summing on $S_{r\bullet}$ exclusively seems also working like with the constant case. More importantly the calculation on the sample only gives the greatest result.

Even if the G-COMP 0 degree model is unbiased, in every case it does not give efficient RMSE (always greater than the benchmark).

Only a few of the estimators give an interesting RMSE. Some of the others can give a RMSE about more than twice the RMSE of the benchmark. It is important to chose carefully the used estimator. On the other hand we can observe that the positive gain **is not really high**:

```{r}
temp %>%
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + geom_histogram(aes(x = ratioRMSE)) +
  ggtitle("Ratio RMSE parameter compared to RMSE benchmark")
```

Estimators with a better RMSE than the benchmark that are the most present:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(parameter, invMatrices, calculTotal) %>% 
  summarize(n = n()) %>% 
  arrange(-n)
```

Three of the estimators are **exclusively based** on the data of the sample $S_{r\bullet}$.

And the parameters that offer the worse RMSE compared to the benchmark:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark, -relExpRMSETWBenchmark) %>% 
  slice_max(relRMSEParam, n = 5L)
```

```{r}
rm(temp)
```

## Impact of the standard deviation

We focus on the impact of the standard deviation. Is there estimators that give better RMSE than the benchmark, independently of $\sigma$? The bias is is considered as variable.

```{r}
#| eval: false
parameters <- expand_grid(constBias = FALSE,
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = "eq",
                          sdY = c(0.1, 1.0, 2.0, 5.0, 10.0, 20.0, 40.0, 60.0))

resultsSD <- loop_simulation(parameters, M = M)
```

```{r}
temp <- resultsSD %>% 
  filter(trueEstimator) %>% 
  select(-YintLaw, -YtelLaw, -sampling, -MBtype, -probSelect) %>% 
  group_by(signAge, sd, phi, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(signAge, sd) %>% 
  arrange(relRMSEParam, .by_group = TRUE) %>% 
  ungroup()
```

```{r}
temp %>% 
  group_by(signAge, sd) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  mutate(percRMSE = 100.0 * relRMSEParam / relEstRMSEBenchmark, .after = relEstRMSEBenchmark) %>% 
  arrange(percRMSE, .by_group = TRUE)
```

We can observe there is always estimators that are better than the benchmark (but sometimes not that much), for any couple (`signAge`, `sd`). In particular, the estimator `HT-samples-sample` is always present. Our estimators are still approximately unbiased but their variance increase with $\sigma$.

For each couple (signAge, sd), we draw the best estimator:

```{r}
temp %>% 
  group_by(signAge, sd) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  mutate(percRMSE = 100.0 * relRMSEParam / relEstRMSEBenchmark, .after = relEstRMSEBenchmark) %>% 
  slice_min(percRMSE)
```

For low values of $\sigma$ the best estimator is HT-population-sample. For $\sigma \geq 10$ it is best to use the 1 degree G-Computation model. However with $\sigma \geq 10$ the gain is really weak.

Trajectories are similar for the both values of `signAge`, even if for small values of $\sigma$ the ratios are quite higher for `signAge = plus`. It seems that the ratio convergences to a value that is right under 1.

Best estimators depending on `signAge` and $\sigma$:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  group_by(signAge, sd) %>% 
  slice_min(ratioRMSE) %>% 
  select(signAge, sd, phi, parameter, invMatrices, calculTotal, ratioRMSE)
```

The greatness of an estimator does not depend on `signAge` (if it is the best for `signAge = minus`, it is the best for `signAge = plus`).

The RMSE ratio increases with $\sigma$. For any value of `signAge` and for $\sigma \in \{0,1,2\}$ we have really interesting ratios, while for higher values the ratio is always right under one, which means that in those conditions there are estimators that are better than the benchmark but not that much (if we follows blindly the empirical results).

The estimator needs to be chosen rightfully, specifically pour low values of $\sigma$ : the RMSE can be about 45% higher than the benchmark in the worst case.

```{r}
temp %>% 
  group_by(signAge, sd) %>% 
  summarize(minRatioRMSE = min(relRMSEParam / relEstRMSEBenchmark),
            maxRatioRMSE = max(relRMSEParam / relEstRMSEBenchmark))
```

If we focus on `HT-samples-sample`, we have the following results:

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + geom_point(aes(x = sd, y = ratioRMSE, color = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation (HT-samples-sample)")
```

And if we draw the best result we have with all estimators:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  group_by(signAge, sd) %>% 
  slice_min(ratioRMSE) %>% 
  select(signAge, sd, phi, parameter, invMatrices, calculTotal, ratioRMSE) %>% 
  ggplot() + geom_point(aes(x = sd, y = ratioRMSE, color = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation (best estimator)")
```

## Impact of $\phi$

We propose different values for $\phi$, in the Gaussian case with variable bias and variable $\sigma$:

```{r}
#| eval: false
parameters <- expand_grid(constBias = FALSE,
                          sampling = "SRS",
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = c("eq", "1/3", "2/3",
                                  "var", "linear", "split3"),
                          sdY = c(1.0, 2.0, 5.0))

resultsPhi <- loop_simulation(parameters, M = M)
```

```{r}
temp <- resultsPhi %>% 
  filter(trueEstimator) %>% 
  select(-YintLaw, -YtelLaw, -sampling, -MBtype, -probSelect) %>% 
  group_by(signAge, sd, phi, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(signAge, sd, phi) %>% 
  arrange(relRMSEParam, .by_group = TRUE) %>% 
  ungroup()
```

List of the estimators and their RMSE ratio depending on signAge, sigma and phi :

```{r}
temp %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(signAge, sd, parameter, invMatrices, calculTotal) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  select(-relError, -relErrorBenchmark, 
         -relRMSEParam, -relEstRMSEBenchmark,
         -relExpRMSETWBenchmark) %>% 
  arrange(ratioRMSE, .by_group = TRUE)
```

We remove the linear, var and split3 options because they give bad results.

Here we print the minimum and maximum RMSE ratios obtained with each situation and each estimator that gives at least one result with an RMSE better than the benchmark:

```{r}
temp %>% 
  filter(!phi %in% c("var", "linear", "split3")) %>% 
  group_by(signAge, sd, parameter, invMatrices, calculTotal) %>% 
  filter(relRMSEParam < relEstRMSEBenchmark) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  select(-relError, -relErrorBenchmark, 
         -relRMSEParam, -relEstRMSEBenchmark,
         -relExpRMSETWBenchmark) %>% 
  summarize(minRMSE = min(ratioRMSE), 
            maxRMSE = max(ratioRMSE), 
            diffMaxMin = maxRMSE - minRMSE,
            percMaxMin = 100.0 * (diffMaxMin) / maxRMSE) %>% 
  group_by(signAge, sd) %>% 
  arrange(minRMSE, .by_group = TRUE)
```

List of the best(s) estimator(s) (for the RMSE) for each couple (`signAge`, `sd`):

```{r}
temp %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(signAge, sd) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  select(-relError, -relErrorBenchmark, 
         -relRMSEParam, -relEstRMSEBenchmark,
         -relExpRMSETWBenchmark) %>% 
  slice_min(ratioRMSE)
```

For any situation the best estimator is `(HT-population-sample)`, with $\phi_k \equiv \frac{2}{3}$. Like previously the RMSE ratio increases with $\sigma$.

Evolution of the RMSE ratio depending on $\sigma$, $\phi$ and `signAge`, for the estimator `(HT-population-sample)`:

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "population", calculTotal == "sample") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + 
  geom_point(aes(x = sd, y = ratioRMSE, color = phi, shape = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation and phi (HT-population-sample)")
```

Evolution of the RMSE ratio depending on $\sigma$, $\phi$ and `signAge`, for the estimator `(HT-samples-sample)`:

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + 
  geom_point(aes(x = sd, y = ratioRMSE, color = phi, shape = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation and phi (HT-samples-sample)")
```

`var` (i.e. a random choice for $\phi_k$) and `split3` gives really bad results. So we will remove it from now on.

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  filter(phi != "var", phi != "split3") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + 
  geom_point(aes(x = sd, y = ratioRMSE, color = phi, shape = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation and phi")
```

The "linear" possibility gives bad results (with ratio stricly superior to 1). The choice of the constant depends of the situation. However, the difference between the ratios is not extremely important.

Note: There is a superposition of "eq", 1/3 and 2/3 for any $\sigma$ with `signAge=plus`.

## Bias detection

We would like to check if, in the case of a constant bias, the estimator

$$\frac{1}{||\phi||_1}(\hat{t}_{pq\phi1}-\hat{t}_{pq\phi2})$$

detects the bias, in the sense of its expected value is equal to the constant bias $\delta$. Moreover, we would like to be sure that this estimator tends to a normal law.

Here we use the empirical standard deviation based on our $M$ observations.

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", 
         MBtype == "variable", signAge == "plus", 
         trueEstimator) %>% 
  select(experiment, normEstPhiBiasDiffHT) %>% 
  group_by(experiment) %>% 
  slice_head() %>% 
  ungroup() %>% 
  select(normEstPhiBiasDiffHT)

estimBias <- temp$normEstPhiBiasDiffHT
```

The expected value is equal to

$$\frac{1}{||\phi||_1}(||\phi||_1\delta_0 + \sum_{k \in U} \phi_k. age_k. \delta_{age} + \sum_{k \in U} \phi_k .sexe_k. \delta_{sex})$$

In our situation we have that $\sum_{k \in U} age_k. \delta_{age} + \sum_{k \in U}.sexe_k. \delta_{sex} = 0$, by construction. So if the $\phi_k$ are constant (which is the case here), the expected value simply becomes $\delta_0$, which is equal to $1$.

```{r}
empMean <- mean(estimBias)
glue("empirical mean: {empMean} (expected: {1})")
```

The empirical mean is close to the nominal value, even with approximately unbiased estimators.

For $\alpha = 0.05$, we have

```{r}
empSD <- sd(estimBias)

alpha <- 0.05
CIlower <- estimBias - qnorm(1.0 - alpha / 2.0) * empSD
CIupper <- estimBias + qnorm(1.0 - alpha / 2.0) * empSD

coverage <- mean(CIlower <= 1.0 & 1.0 <= CIupper)

glue("coverage: {coverage} (expected: {1 - alpha})")
```

the coverage is very close to the target.

We check the normality of our estimator:

```{r}
temp %>% 
ggplot() + 
  geom_density(aes(x = normEstPhiBiasDiffHT)) + 
  geom_vline(aes(xintercept = 1.0, colour = "expected"), show.legend = TRUE) +
  ggtitle("Detection of a variable bias (average)") +
  xlab("normalized HT-difference estimator") +
  scale_color_manual(name = "bias", values = c(expected = "red"))


ks.test(x = (estimBias - 1.0) / empSD, y = "pnorm")
```

With an important p-value the Kolmogorov-Smirnov test ensure that our estimator is normal.

# More general cases

Percentage of values for $z_k^T\beta_{tel}$ that are $< 0$ with `signAge=minus`:

```{r}
glue("{sum(Z %*% betaTel < 0.0) * 100.0 / N} %")
```

```{r}
#| eval: false

# Grid of all evaluated parameters
parameters <- expand_grid(constBias = c(TRUE, FALSE),
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = c("gaussian", "exponential"),
                          YintLaw = c("gaussian", "exponential", "split3"),
                          signAge = c("plus", "minus"),
                          phi = c("eq", "var"),
                          sdY = c(1.0, 2.0, 5.0)) %>%
  # If there is only exponential laws the parameter sdY is of no interest
  mutate(sdY =
           ifelse(YtelLaw == "gaussian" |
                    YintLaw == "gaussian", sdY, NA_real_)) %>% 
  filter(YtelLaw != "gaussian" | YintLaw != "gaussian")

resultsGeneral <- loop_simulation(parameters)
```

## Telephone total estimation

The size of the grid of the hyperparameters (and so the number of cases) is huge, so each case cannot be studied individually.

Note : in the case of an exponential law for $Y_{int}$ or $Y_{tel}$, the standard deviation parameter is not considered (the standard deviation will be equal to $(z_k^T\beta)^2$, with the adequate $\beta$).

```{r}
temp <- resultsGeneral %>% 
  filter(trueEstimator) %>% 
  group_by(YintLaw, YtelLaw, sd, signAge, MBtype, phi,
           sampling, parameter, invMatrices, calculTotal)
```

Here we have the optimal relative RMSE for each condition:

```{r}
temp <- temp %>% 
  summarise(relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relDiffBenchmarkMean = round(mean(diffBenchmark / abs(expYtel)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L)) %>% 
  ungroup() 

temp %>% 
  group_by(sd, YintLaw, YtelLaw, signAge, MBtype, sampling) %>% 
  slice_min(relRMSEParam)
```

Number of situations that possess a better RMSE with some parameter different than the benchmark:

```{r}
tempBis <- temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)

nbExperiments <- n_groups(tempBis)

nbSuccesses <- tempBis %>% 
  summarise(nbEstimators = sum(relRMSEParam <= relEstRMSEBenchmark),
            bestRelRMSE = min(relRMSEParam),
            relEstRMSEBenchmark = mean(relEstRMSEBenchmark)) %>% 
  arrange(bestRelRMSE / relEstRMSEBenchmark) %>% 
  filter(nbEstimators > 0L) %>% 
  nrow()
  
glue("{nbSuccesses} / {nbExperiments} with at least one interesting estimator")

rm(tempBis)
```

Estimators that are the most present (which have a lower value of RMSE compared to the benchmark, not necessarily the lowest):

```{r}
temp %>% 
  select(-relError, -relDiffBenchmarkMean) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(phi, parameter, invMatrices, calculTotal) %>% 
  summarize(n = n(), ratio = n / nbExperiments) %>% 
  arrange(-n)
```

Case when HT-sample-sample is the best:

```{r}
temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  slice_min(relRMSEParam) %>% 
  filter(parameter == "HT", 
         invMatrices == "samples",
         calculTotal == "sample") %>% 
  distinct(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)
```

This estimator seems to be optimal only is the measure effect is conditionally constant.

Case when HT-population-population is the best:

```{r}
temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  slice_min(relRMSEParam) %>% 
  filter(parameter == "HT", 
         invMatrices == "population",
         calculTotal == "population") %>% 
  distinct(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)
```

```{r}
rm(temp)
```

## Variances

Here we focus on the special case when selection probabilities are known and the estimator is Horvitz-Thompson with the complete inverse matrices and the summation for $\hat{t}_{\phi\Delta}$ is made on the entire population. The goal is to compare the estimated (co)variances with the ones expected by the theory.

```{r}
temp <- resultsGeneral %>% 
  filter(parameter == "HT", probSelect == "true", 
         invMatrices == "population", calculTotal == "population")


# tempBis <- temp %>% 
#   filter(sampling == "SRS",
#          YintLaw == "gaussian", YtelLaw == "gaussian",
#          phi == "eq", sd == "1", MBtype == "variable",
#          signAge == "plus")
```

Before comparison, we can observe on the Gaussian case (variable measure effect, positive coefficient for age on telephone, $\phi_k \equiv \frac{1}{2}$) with $\sigma =1$ that the variance do not stabilize completely:

```{r}
# sumSqErrors <- cumsum(tempBis[, c("diffPhiBias", "diffYtelEstMP")]^2L)
# 
# RMSEs <- 
#   data.frame(totPhiDelta =  sumSqErrors[-1L, "diffPhiBias"] / Kvec[-1L],
#              totYtel = sumSqErrors[-1L, "diffYtelEstMP"] / Kvec[-1L],
#              M = Kvec[-1L])
# 
# RMSEs %>% 
#   pivot_longer(cols = c("totPhiDelta", "totYtel"), 
#                names_to = "target", 
#                values_to = "estRMSE") %>% 
#   ggplot() +
#   geom_point(aes(x = M, y = estRMSE, colour = target)) +
#   ggtitle("Estimated RMSE depending on the number of evaluations (M)")
# 
# rm(tempBis, RMSEs)
```

```{r}
temp %>% 
  group_by(sampling, YintLaw, YtelLaw, sd, MBtype, signAge, phi) %>% 
  summarise(M = n(),
            estSDEstimator = sd(estYtelTrueMP),
            expSDEstimator = sqrt(mean(expVar2)),
            estVarPhi1 = var(estPhiYintTrueMP),
            expVarPhi1 = mean(expVarPhi1),
            estVarPhi2 = var(estPhiBarYtelTrueMP),
            expVarPhi2 = mean(expVarPhi2),
            estCovarphi12 = cov(estPhiYintTrueMP, estPhiBarYtelTrueMP),
            expCovarPhi12 = mean(expCovarPhi12),
            estCovarphi1Delta = cov(estPhiYintTrueMP, estPhiBias),
            expCovarPhi1Delta = mean(expCovarPhi1Delta),
            estVarPhiDelta = var(estPhiBias),
            expVarPhiDelta = mean(expVarPhiDelta),
            estCovarphi2Delta = cov(estPhiBarYtelTrueMP, estPhiBias),
            expCovarPhi2Delta = mean(expCovarPhi2Delta))
```

```{r}
rm(temp)
```
