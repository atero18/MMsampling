---
title: "Measure bias correction (linear case)"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

```{r}
#| message: false
#| warning: false
library(dplyr)
options(dplyr.summarise.inform = FALSE)
library(tidyr)
library(glue)
library(tibble)
library(parallel)
library(tictoc)
library(ggplot2)
library(Matrix)
library(progress)
library(marginaleffects)
```

```{r}
devtools::load_all()
```

Linear models on both modes

# Building constants

```{r}
set.seed(145L)
```

```{r}
N <- 1000L
phi <- rep(0.5, N)
```

Creating independent covariates

```{r}
p <- 1L

sex <- sample(c(0L, 1L), replace = TRUE, size = N, prob = c(0.5, 0.5))
age <- rnorm(n = N, mean = sex * 41.1 + (1L - sex) * 43.9, sd = 10.0)
age[age < 0.0] <- 0.0

Z <- data.frame(const = 1.0, 
                age = age, 
                sex = sex)

rm(age)
sex <- factor(sex)
levels(sex) <- c("Woman", "Man")
```

```{r}
summary(Z)
```

```{r}
data <- Z[, c("sex", "age")]
data$sex <- sex
ggplot(data) + 
  geom_density(aes(x = age, colour = sex)) +
  ggtitle("Age ~ sex")
rm(data)
```

```{r}
Z <- as.matrix(Z)
```

Creating parameters for two modes : "int" and "tel". `delta` is defined such that the average measure bias is equal to 1. It depends on `Z`.

```{r}
betaTel <- c(1.0, -0.1, 4.0)
print(betaTel)
delta <- c(1.0, -2.0 * mean(Z[, "age"])^-1L, 2.0 * N / sum(Z[, "sex"]))
betaInt <- betaTel + delta
  

print(betaInt)
```

$Y_{int}$ and $Y_{tel}$ expectations depending on `Z`:

```{r}
data.frame(tel = Z %*% betaTel, int = Z %*% betaInt) %>% 
  pivot_longer(cols = c("tel", "int"), names_to = "mode", values_to = "value") %>% 
  ggplot() + geom_density(aes(x = value, colour = mode))

glue("average value for telephone: {mean(Z %*% betaTel)}")
glue("average value for internet: {mean(Z %*% betaInt)}")
```

```{r}
totBias <- sum(Z %*% delta)
glue("total bias on U: {totBias}")
meanBias <- totBias / N
glue("average bias on U: {meanBias}")
```

We simulate a mode selection MAR (Missing At Random) under a logistic model, with $Z$ as variable:

```{r}
# With MAR

expit <- function(x) 1.0 / (1.0 + exp(-x))

alphaInt <- c(0.7, -0.02, -0.5)
pInt <- expit(Z %*% alphaInt) %>% as.vector()

alphaTel <- c(0.3, 0.02, -0.5)
pTel <- expit(Z %*% alphaTel) %>% as.vector()
```

Here are some details about the $p_{int}$ and $p_{tel}$:

```{r}
data.frame(mode = factor(rep(c("int", "tel"), each = N)), 
           prob = c(pInt, pTel)) %>% 
  ggplot() + 
  geom_density(aes(x = prob, color = mode)) +
  ggtitle("Distribution of p_int and p_tel")
```

```{r}
pInt[sex == "Man"] %>% summary()
pInt[sex == "Woman"] %>% summary()
```

```{r}
cat("internet probabilities:\n")
summary(pInt) %>% print()
cat("telephone probabilities:\n")
summary(pTel) %>% print()
```

# Simulation functions

Randomly affect a mode (or non-response) to each unit

```{r}
gen_choice_bimode <- function(I, p1, p2, mode1 = "int", mode2 = "tel")
{
  N <- length(p1)
  
  R1 <- runif(N) <= p1
  R2 <- runif(N) <= p2
  
  modes <- rep("nr", N)
  modes[R1] <- mode1
  
  modes[!R1 & R2] <- mode2
  
  modes[!I] <- "nr"
  
  modes
}
```

Parameters:

-   `meanZ` and `varZ` if `Z` is variable (i.e. `fixZ == FALSE`). constant Z can be given with `Z`
-   $\alpha_{tel}$ and $\alpha_min$, coefficients of conditional expectations of $r_{tel}$ and $r_{int}$
-   $\beta_{tel}$ and $\beta_{int}$, coefficients of conditional expectations of $Y_{tel}$ and $Y_{int}$
-   the sampling type (`sampling`)
-   the conditional standard deviations of $Y_{tel}$ and $Y_{int}$ (`sdInt` and `sdTel`)
-   the law of $Y_{tel}$ (`YtelLaw`)
-   the law of $Y_{int}$ (`YintLaw`)
-   the state of $\phi$ (`phi`)
-   the number of iterations (`K`)
-   the random seed (`seed`)

```{r}
simulationUncounf <- 
  function(N,
           meanZ = numeric(length(alphaInt)), 
           covarZ = diag(1.0, length(alphaInt)), 
           sampling = "SRS",
           alphaInt, alphaTel, 
           betaInt, betaTel, 
           YintLaw = "gaussian", YtelLaw = "gaussian",
           sdInt = 1.0, sdTel = 1.0, 
           phi = rep(0.5, N), K = 1000L,
           Z = NULL,
           n = ceiling(N / 4L),
           seed = 123L,
           cluster = NULL)
  {
    
    stopClusterAtEnd <- is.null(cluster)
    # Z is exported only if the cluster is created in the function
    if (is.null(cluster))
    {
      nbCores <- detectCores() - 1L
      cluster <- makeCluster(nbCores)
      
      if (is.null(Z))
      Z <- mvtnorm::rmvnorm(N, mean = meanZ, sigma = covarZ, checkSymmetry = TRUE)
    
      clusterExport(cluster, 
                    varlist = "Z",
                    envir = environment())
    }
    
    
    # Making clusters random-independants
    clusterSetRNGStream(cl = cluster, iseed = seed)
    
    clusterEvalQ(cluster, library(dplyr))
    clusterEvalQ(cluster, library(Matrix))
    clusterEvalQ(cluster, library(tibble))
    clusterEvalQ(cluster, library(marginaleffects))
    clusterEvalQ(cluster, library(MMsampling))
    
    clusterExport(cluster, varlist = c("sampling", 
                                       "betaInt", "betaTel", 
                                       "YintLaw", "YtelLaw", 
                                       "sdInt", "sdTel", 
                                       "n"),
                  envir = environment())
    
    expit <- function(x) 1.0 / (1.0 + exp(-x))

    pInt <- expit(Z %*% alphaInt) %>% as.numeric()
    pq1Mat <- pInt %*% t(pInt)
    diag(pq1Mat) <- pInt
    
    pTel <- expit(Z %*% alphaTel) %>% as.numeric()
    pq2Mat <- pTel %*% t(pTel)
    diag(pq2Mat) <- pTel
    
    clusterExport(cluster, 
                  varlist = c("pInt", "pq1Mat", "pTel", "pq2Mat"),
                  envir = environment())
    

    if (sampling == "SRS")
    {
      pi <- rep(n / N, N)
      piMat <- matrix(n * (n - 1L) / (N * (N - 1L)), nrow = N, ncol = N)
      diag(piMat) <- n / N
      
      clusterExport(cluster, 
                    varlist = c("pi", "piMat"),
                    envir = environment())
    }
    

    if (YtelLaw == "gaussian")
    {
      expYtels <- as.vector(Z %*% betaTel)
      
      if (sdTel == "split3")
      {
        quantilesTel <- quantile(expYtels, probs = c(1.0 / 3.0, 2.0 / 3.0))
        sdTels <- rep(3.0, N)
        sdTels[sdTels <= quantilesTel[2L]] <- 2.0
        sdTels[sdTels <= quantilesTel[1L]] <- 1.0
      }
      else
      {
        sdTel <- as.numeric(sdTel)
        sdTels <- rep(sdTel, N)
      }
      
      covarYtel <- diag(sdTels^2L)
        
    }
    else if (YtelLaw == "exponential")
    {
      expYtels <- as.vector(abs(Z %*% betaTel))
      covarYtel <- diag(expYtels^2L)
    }
    
    if (YintLaw == "gaussian")
    {
      expYints <- as.vector(Z %*% betaInt)
      
      if (sdInt != "split3")
      {
        sdInt <- as.numeric(sdInt)
        sdInts <- rep(sdInt, N)
        covarYint <- diag(sdInts^2L)
      }
    }
    else if (YintLaw == "exponential")
    {
      expYints <- as.vector(abs(Z %*% betaInt))
      covarYint <- diag(expYints^2L)
    }
    else if (YintLaw == "split3")
    {
      expYints <- as.vector(Z %*% betaInt)
      
      quantilesTel <- quantile(expYtels, probs = c(1.0 / 3.0, 2.0 / 3.0))
      
      # Between first and second quantile of 3rd degree we add one
      expYints[expDeltas > quantilesInt[1L]] <- 
        expYints[expDeltas > quantilesInt[1L]] + 1.0
      
      # After the second quantile of 3rd degree we add one more
      expYints[expDeltas > quantilesInt[2L]] <- 
        expYints[expDeltas > quantilesInt[2L]] + 1.0
    }
    
    if (sdInt %in% c("gaussian", "split3") && sdInt == "split3")
    {
      quantilesInt <- quantile(expYints, probs = c(1.0 / 3.0, 2.0 / 3.0))
      sdInts <- rep(3.0, N)
      sdInts[sdInts <= quantilesInt[2L]] <- 2.0
      sdInts[sdInts <= quantilesInt[1L]] <- 1.0
      covarYint <- diag(sdInts^2L)
    }
    
    clusterExport(cluster,
                  varlist = c("expYtels", "sdInts", "sdTels",
                              "covarYtel", 
                              "expYints", "covarYint"),
                  envir = environment())
    
    clusterExport(cluster, "gen_choice_bimode")
      
    
    if (class(phi) == "character")
      phiType <- phi
    else
      phiType <- "vector"
    
    if (phi == "eq")
      phi <- rep(0.5, N)
    
    else if (phi == "1/3")
      phi <- rep(1.0 / 3.0, N)
    
    else if (phi == "2/3")
      phi <- rep(2.0 / 3.0, N)
  
    else if (phi == "var")
      phi <- seq_len(N) / N
    
    # When there is no bias, phi = 0.5
    # When the bias increase (in absolute), phi tends to zero
    # (it gives more weights to the unbiased value)
    else if (phi == "linear")
    {
      expDeltas <- expYints - expYtels
      maxBias <- max(abs(expDeltas))
      phi <- 0.5 - 0.5 * abs(expDeltas) / maxBias
    }
    else if (phi == "split3")
    {
      expDeltas <- expYints - expYtels
      absExpDeltas <- abs(expDeltas)
      quantiles <- quantile(absExpDeltas, c(1.0 / 3.0, 2.0 / 3.0))
      phi <- rep(0.5, N)
      phi[absExpDeltas >= quantiles[1L]] <- 2.0 / 3.0
      phi[absExpDeltas >= quantiles[2L]] <- 1.0
    }
      
    
    clusterExport(cluster, 
                  varlist = "phi", 
                  envir = environment())
    
    monoSim <- function(...)
    {
      nbExperiments <- 12L
      
      partialResults <- data.frame(parameter = character(nbExperiments),
                                   trueEstimator = logical(nbExperiments),
                                   probSelect = character(nbExperiments),
                                   invMatrices = character(nbExperiments),
                                   calculTotal = character(nbExperiments),
                                   estPhiBias = numeric(nbExperiments))
      
      row <- 1L
      
      p <- ncol(Z)
        
      
      if (sampling == "SRS")
      {
        selectedSample <- sample(seq_len(N), size = n, replace = FALSE)
        
        I <- logical(N)
        I[selectedSample] <- TRUE
        rm(selectedSample)
      }
      
      # Mode selection simulation
      modeChoiceOK <- FALSE
      
      while (!modeChoiceOK)
      {
        # modeChoiceOK <- TRUE
        
        modes <- gen_choice_bimode(I, pInt, pTel)
        trueProbsSelect <- rep(NA_real_, N)
        maskInt <- modes == "int"
        maskTel <- modes == "tel"
        trueProbsSelect[maskInt] <- pInt[maskInt]
        trueProbsSelect[maskTel] <- ((1.0 - pInt) * pTel)[maskTel]
        
        if (rankMatrix(Z[maskInt, , drop = FALSE]) == p &&
            rankMatrix(Z[maskTel, , drop = FALSE]) == p)
          modeChoiceOK <- TRUE
        else
          warning("Singular Zint / Ztel")
        
      }
      
      
      trueWeights <- (pi * trueProbsSelect)^-1L
    
      # Estimation of mode selection probabilities
      estimProbsSelect <- 
        MMsampling::estim_response_prob_sequential(I, Z,
                                                   modes, 
                                                   c("int", "tel"))$unconditional
    
      estimWeights <- (pi * estimProbsSelect)^-1L
      
      
       # Affecting values to Y_int and Y_tel
      if (YtelLaw == "gaussian")
      {
        Ytels <- expYtels + rnorm(n = N, sd = sdTels)
      }
        
      
      else if (YtelLaw == "exponential")
        Ytels <- rexp(n = N, rate = expYtels^-1L)
      
      expTotYtel <- sum(expYtels)
      trueYtel <- sum(Ytels)
      
      if (YintLaw %in% c("gaussian", "split3"))
        Yints <- expYints + rnorm(n = N, sd = sdInts)

      else if (YintLaw == "exponential")
        Yints <- rexp(n = N, rate = expYints^-1L)
      
      deltas <- Yints - Ytels
      
      # Evaluating the measure bias total (random value)
      trueTotalBias <- crossprod(phi, deltas) %>% as.vector()
      expPhiBias <- crossprod(phi, expYints - expYtels) %>% as.vector()
      
      # Creating the observed outcome vector
      Yobs <- rep(NA_real_, N)
      Yobs[maskInt] <- Yints[maskInt]
      Yobs[maskTel] <- Ytels[maskTel]
      
      sample <- MMsampling:::MMSample$new(Z = Z, pi = pi, I = I, 
                             modes = modes, Yobs = Yobs, phi = phi)
      
      # Full HT estimation, with full population matrix (Z^tZ)^-1 and
      # known probabilities
      resEval <-
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes,
                               "int", "tel", "HT", "HT", pi,
                        trueProbsSelect, sampleMatrix = FALSE) %>%
        estim_MB_by_MCO(Z, phi = phi) %>%
        sum()
      
  
      partialResults[row,] <- 
        c(parameter = "HT", trueEstimator = FALSE,
          probSelect = "true", invMatrices = "population",
          calculTotal = "population", estPhiBias = resEval)
      
      row <- row + 1L
  
      # Full HT estimation, with sample matrix (Z_S^TZ_S)^-1 and
      # known probabilities
      resEval <-
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes,
                               "int", "tel", "HT", "HT", pi,
                        trueProbsSelect, sampleMatrix = TRUE) %>%
        estim_MB_by_MCO(Z, phi = phi) %>%
        sum()
  
      partialResults[row,] <- c(parameter = "HT", trueEstimator = FALSE,
                                probSelect = "true", invMatrices = "samples", 
                                calculTotal = "population", estPhiBias = resEval)
      
      row <- row + 1L
      

      # Estimation on a unique MCO for Y1 and Y2, with beta X and only a constant
      # delta for the estimation of the measure bias
      deltaEval <- 
        estim_delta_MCO_unique_model_const(Z, Yobs, modes, "int", "tel")
      resEvalFull <- sum(phi) * deltaEval
      resEvalHT <- 
        as.vector(crossprod(phi[maskInt],
                            estimWeights[maskInt]) * 
                    deltaEval)
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_0_deg",
                   trueEstimator = TRUE,
                   probSelect = "null",
                   invMatrices = "null",
                   calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      
      row <- row + 2L
      
      
      # G-COMP first degree estimation, <=> double MCO estimation
      # tic()
      # estimdelta <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
      #                   "int", "tel", "MCO", "MCO", pi, NULL)
      estMBs <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                                "int", "tel", "G-COMP", "G-COMP", pi, NULL, 
                                returnMB = TRUE, order = 1L)
      
      resEvalFull <- crossprod(phi, estMBs) %>% as.vector()
      
      resEvalHT <- crossprod(phi[maskInt] * estimWeights[maskInt],
                             estMBs[maskInt]) %>% as.vector()
      # toc() %>% print()
      
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_1_deg", trueEstimator = TRUE,
                   probSelect = "null", 
                   invMatrices = "null", calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # G-COMP two degrees estimation
      # tic()
      estMBs <- estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                                "int", "tel", "G-COMP", "G-COMP", pi, NULL, 
                                returnMB = TRUE, order = 2L)
      
      resEvalFull <- crossprod(phi, estMBs) %>% as.vector()
      
      resEvalHT <- crossprod(phi[maskInt] * estimWeights[maskInt],
                             estMBs[maskInt]) %>% as.vector()
      # toc() %>% print()
      
      
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "G-COMP_2_deg", trueEstimator = TRUE,
                   probSelect = "null", 
                   invMatrices = "null", calculTotal = c("population", "sample"),
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      
      
      # Full HT estimation, with full population matrix (Z^tZ)^-1 and
      # unknown probabilities
      # tic()
      estimdelta <- 
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                        "int", "tel", "HT", "HT", pi, 
                        estimProbsSelect, sampleMatrix = FALSE)
      
      resEvalFull <- estimdelta %>% 
        estim_MB_by_MCO(Z, phi = phi) %>% 
        sum()
      
      resEvalHT <- estimdelta %>% 
        estim_MB_by_MCO(Z, 
                        phi = phi, 
                        weights = estimWeights,
                        mask = maskInt) %>% 
        sum()
    
      # toc() %>% print()
     
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "HT", trueEstimator = TRUE,
                   probSelect = "estimation", invMatrices = "population", 
                   calculTotal = c("population", "sample"), 
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # Full HT estimation, with sample matrix (Z_S^TZ_S)^-1 and
      # unknown probabilities
      # tic()
      estimdelta <- 
        estim_delta_MCO(sample$Z, sample$Yobs, sample$modes, 
                        "int", "tel", "HT", "HT", pi, 
                        estimProbsSelect, sampleMatrix = TRUE) 
      
      
      resEvalFull <- estimdelta %>% 
        estim_MB_by_MCO(Z, phi = phi) %>% 
        sum()
      
      resEvalHT <- estimdelta %>% 
        estim_MB_by_MCO(Z, 
                        phi = phi, 
                        weights = estimWeights, 
                        mask = maskInt) %>% 
        sum()
    
      # toc() %>% print()
      
      partialResults[c(row, row + 1L),] <- 
        data.frame(parameter = "HT", trueEstimator = TRUE,
                   probSelect = "estimation", invMatrices = "samples", 
                   calculTotal = c("population", "sample"), 
                   estPhiBias = c(resEvalFull, resEvalHT))
      
      row <- row + 2L
      
      # Estimations of t_phi1 and t_phi2
      # - With true weights
      estTotPhiYintHTTrueMP <- 
        phi[maskInt] * 
        trueWeights[maskInt] * 
        Yints[maskInt]
      
      estTotPhiYintHTTrueMP <- sum(estTotPhiYintHTTrueMP)
      
      estTotPhiYtelHTTrueMP <- 
        (1.0 - phi[maskTel]) * 
        trueWeights[maskTel] * 
        Ytels[maskTel]
      
      # - With estimated weights
      estTotPhiYtelHTTrueMP <- sum(estTotPhiYtelHTTrueMP)
    
      estTotPhiYintHTEstMP <-
        phi[maskInt] *
        estimWeights[maskInt] *
        Yints[maskInt]
    
      estTotPhiYintHTEstMP <- sum(estTotPhiYintHTEstMP)
    
      estTotPhiYtelHTEstmMP <-
        (1.0 - phi[maskTel]) *
        estimWeights[maskTel] *
        Ytels[maskTel]
    
      estTotPhiYtelHTEstmMP <- sum(estTotPhiYtelHTEstmMP)
      
      # Don't know why but some columns are
      #  automatically converted as character
      partialResults$estPhiBias <- as.numeric(partialResults$estPhiBias)
      partialResults$trueEstimator <- as.logical(partialResults$trueEstimator)
      # Value estimated only on telephone answers, with estimated selection probabilities
      benchmark <- crossprod(estimWeights[maskTel], Ytels[maskTel]) %>% 
        as.vector()
      trueWeightsBenchmark <- crossprod(trueWeights[maskTel], Ytels[maskTel]) %>% 
        as.vector()

      
      partialResults <- partialResults %>% 
        add_column(.before = 1L, expn = sum(pi)) %>% 
        add_column(.after = "expn", n = sum(I)) %>% 
        add_column(.after = "n", trueYtel = trueYtel) %>% 
        add_column(.after = "trueYtel", expYtel = expTotYtel) %>% 
        add_column(.before = "estPhiBias", truePhiBias = trueTotalBias) %>% 
        add_column(.after = "truePhiBias", expPhiBias = expPhiBias) %>% 
        add_column(.after = "expPhiBias", 
                   estPhiYintTrueMP = estTotPhiYintHTTrueMP) %>% 
        add_column(.after = "estPhiYintTrueMP", 
                   estPhiYintEstMP = estTotPhiYintHTEstMP) %>% 
        add_column(.after = "estPhiYintEstMP", 
                   estTotPhiYtelTrueMP = estTotPhiYtelHTTrueMP) %>% 
        add_column(.after = "estTotPhiYtelTrueMP", 
                   estTotPhiYtelEstMP = estTotPhiYtelHTEstmMP) %>% 
        mutate(.after = "estTotPhiYtelEstMP",
               estYtelTrueMP = estPhiYintTrueMP + 
                 estTotPhiYtelTrueMP -
                 estPhiBias) %>% 
        mutate(.after = "estYtelTrueMP", 
               estYtelEstMP = estPhiYintEstMP + 
                 estTotPhiYtelEstMP -
                 estPhiBias) %>% 
        add_column(.after = "estYtelEstMP", benchmark = benchmark) %>% 
        add_column(.after = "benchmark", TWBenchmark = trueWeightsBenchmark)
        
      #results <- rbind(results, partialResults)
    }
    
    clusterExport(cluster, 
                  varlist = "monoSim", 
                  envir = environment())
    
    #browser()
    
    #results <- lapply(X = seq_len(K), FUN = monoSim)
    results <- parLapply(cluster, X = seq_len(K), fun = monoSim)
    
    if (stopClusterAtEnd)
      stopCluster(cluster)
      


    parameters <- results[[1L]] %>% 
    select(parameter, probSelect, invMatrices, calculTotal)
    
    parameters[, c("expVar2", "expVarPhi1", "expVarPhi2", 
                   "expCovarPhi12", "expVarPhiDelta", 
                   "expCovarPhi1Delta", "expCovarPhi2Delta")] <- NA_real_
  
    for (l in seq_len(nrow(parameters)))
    {
      parameter <- parameters[l, ]
      
      if (parameter$parameter == "HT" && 
          parameter$probSelect == "true" &&
          parameter$invMatrices == "population" && 
          parameter$calculTotal == "population")
      {
        expVars <- var_estim_tot_BM(modeTotBiased = "HT", modeTotRef = "HT",
                                    calculTotal = "population",
                                    expY1 = expYints, expY2 = expYtels,
                                    covarY1 = covarYint, covarY2 = covarYtel,
                                    piMat = piMat,
                                    pq1Mat = pq1Mat, pq2Mat = pq2Mat,
                                    phi = phi,
                                    subResults = TRUE)
        
        parameters[l, names(expVars)] <- expVars
      }
    }
    
    
    results <- do.call("rbind", results)
    
    if (any((betaInt - betaTel)[-1L] != 0.0))
        MBtype <- "variable"
      else
        MBtype <- "constant"
    
    results <- results %>% 
      add_column(.before = "n", sampling = sampling) %>% 
      add_column(.before = "sampling", N = N) %>% 
      add_column(.after = "n", YtelLaw = factor(YtelLaw)) %>% 
      add_column(.after = "YtelLaw", sdTel = sdTel) %>% 
      add_column(.after = "YtelLaw", YintLaw = factor(YintLaw)) %>% 
      add_column(.after = "YintLaw", sdInt = sdInt) %>% 
      add_column(.after = "YintLaw",
                 MBtype = factor(MBtype)) %>% 
      add_column(.after = "MBtype", phi = factor(phiType))
    

    suppressMessages(results <- results %>% inner_join(parameters, by = NULL))
    
      
    # We calculate the real variance of the benchmark estimator
    sdTWBenchmark <- var_HT_seq_phi2(expY2 = expYtels, 
                                     covarY2 = covarYtel,
                                     piMat = piMat,
                                     pq1Mat = pq1Mat,
                                     pq2Mat = pq2Mat,
                                     phi = numeric(N),
                                     correcEstimWeights = TRUE,
                                     Z) %>% sqrt()
    
    results <- results %>% 
      mutate(expSDTWBenchmark = sdTWBenchmark)
    
    
    results
  }
```

-   `trueYtel`: $t_{tel} =\sum_{k \in U} y_{2k}$ for this iteration
-   `expYtel` : $\mathbb{E}[t_{tel}]=\mathbb{E} [\sum_{k \in U} y_{2k}]$
-   `estPhiBias` : $t_{\phi \hat{\Delta y}}$
-   `truePhiBias` : $t_{\phi\Delta y}=\sum_{k\in U} \phi_k \Delta y_k$
-   `expPhiBias` : $\mathbb{E}[t_{\phi\Delta y}]$
-   `estPhiYintTrueMP` : $\hat{t}_{p,\phi y_1} = \sum_{k \in S_r}\frac{\phi_k y_{1k}}{\pi_k p_{1k}}$
-   `estPhiYintEstMP` : $\hat{t}_{\hat{p},\phi y_1} = \sum_{k \in S_r}\frac{\phi_k y_{1k}}{\pi_k \hat{p}_{1k}}$
-   `estYtelTrueMP` : $\hat{t}_{py_2}:= \hat{t}_{p,\phi y_1} + \hat{t}_{p,\phi y_2} - t_{\phi \hat{\Delta y}}$
-   `estYtelEstMP` : $\hat{t}_{\hat{p}y_2}:= \hat{t}_{\hat{p},\phi y_1} + \hat{t}_{\hat{p},\phi y_2} - t_{\phi \hat{\Delta y}}$
-   `expVar2` : $\mathbb{V}[\hat{t}_{\hat{p}y_2}]$ PAS ENCORE VRAI, À CONSIDÉRER
-   `TWBenchmark` : $\sum_{k \in S_{mr}} \frac{y_{2k}}{\pi_k(1-p_{1k})p_{2k}}$
-   `benchmark` : $\sum_{k \in S_{mr}} \frac{y_{2k}}{\pi_k(1-\hat{p}_{1k})\hat{p}_{2k}}$
-   `sdTWBenchmark` : standard deviation of the benchmark
-   `calculTotal` : `population` if we calculate $\hat{t}_{\phi \hat{\Delta y}}$ (imputation on the entire population), `sample` if it is $t_{\phi \hat{\Delta y}}$ (on $S_{r\bullet}$)


```{r}
#| echo: false
loop_simulation <- function(parameters, K = 2000L)
{
  nbCores <- detectCores() - 1L
  cluster <- makeCluster(nbCores)
  
  clusterExport(cluster, 
                varlist = "Z",
                envir = environment())

  results <- NULL
  nbResults <- 0L
  
  parameters <- parameters %>% distinct()
  
  betaTelMinus <- betaTel
  betaIntMinus <- betaInt
  
  # Case when the sign a the age coefficient for telephone is positive
  betaTelPlus <- betaTel
  betaTelPlus[-1L] <- -betaTel[-1L]
  betaIntPlus <- betaInt
  betaIntPlus[-1L] <- -betaInt[-1L]
  
  bar <- progress_bar$new(total = nrow(parameters),
                          format = "[:bar] :current/:total (:percent) eta: :eta")
  
  for (i in seq_len(nrow(parameters)))
  {
    tic()
    expParams <- parameters[i, ]
    #print(expParams)
  
    
    if (expParams$signAge == "plus")
    {
      betaIntTemp <- betaIntPlus
      betaTelTemp <- betaTelPlus
    }
    else if (expParams$signAge == "minus")
    {
      betaIntTemp <- betaIntMinus
      betaTelTemp <- betaTelMinus
    }
    
    if (expParams$constBias)
      betaIntTemp[-1L] <- betaTelTemp[-1L]
    
    expResults <- simulationUncounf(N, K = K,
                                    Z = Z,
                                    sampling = expParams$sampling,
                                    alphaInt = alphaInt,
                                    alphaTel = alphaTel,
                                    betaInt = betaIntTemp,
                                    betaTel = betaTelTemp,
                                    YtelLaw = expParams$YtelLaw,
                                    YintLaw = expParams$YintLaw,
                                    phi = expParams$phi,
                                    sdInt = expParams$sdY,
                                    sdTel = expParams$sdY,
                                    seed = 200L,
                                    cluster = cluster)
    
    expResults <- expResults %>% 
      rename(sd = sdInt) %>% 
      select(-sdTel) %>% 
      mutate(signAge = expParams$signAge)
  
    if (i == 1L)
      expNbResults <- nrow(expResults)
    
    nbResults <- nbResults + expNbResults
    expResults <- expResults %>% 
      add_column(.before = 1L, 
                 experiment = factor(rep(nbResults + seq_len(K), 
                                         each = expNbResults / K)))
    
    results <- rbind(results, expResults)
    
    #t <- toc(quiet = TRUE)
    
    #glue("{round(unname(t$toc - t$tic))}s ({i} / {nrow(parameters)})") %>% print()
    bar$tick()
  }
  
  stopCluster(cluster)
  
  results$signAge <- as.factor(results$signAge)
  results$parameter <- as.factor(results$parameter)
  results$probSelect <- as.factor(results$probSelect)
  results$invMatrices <- as.factor(results$invMatrices)
  results$calculTotal <- as.factor(results$calculTotal)
  
  results <- results %>% 
    mutate(.after = "estPhiBias", diffPhiBias = estPhiBias - truePhiBias) %>% 
    mutate(.after = "estTotPhiYtelEstMP", 
           diffYtelEstMP = estYtelEstMP - trueYtel) %>% 
    mutate(.after = "estTotPhiYtelTrueMP",
           diffYtelTrueMP = estYtelTrueMP - trueYtel) %>% 
    mutate(.after = diffYtelTrueMP, diffBenchmark = benchmark - trueYtel)
  
  results
}
```

# Measure bias evaluation

Evaluation of $K$ experiments:

```{r}
K <- 15L
Kvec <- seq_len(K)
```

```{r}
load(file = "../results.RData")
```

Here we evaluate our estimators on the simplest cases : Gaussian laws with $\phi_k \equiv \frac{1}{2}$, a few values for $\sigma$ and a changing sign for $\beta_{tel,age}$.

```{r}
#| eval: false

# Grid of all evaluated parameters
parameters <- expand_grid(constBias = c(TRUE, FALSE),
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = "eq",
                          sdY = c(1.0, 2.0, 5.0, "split3")) %>%
  # If there is only exponential laws the parameter sdY is of no interest
  mutate(sdY =
           ifelse(YtelLaw == "gaussian" |
                    YintLaw == "gaussian", sdY, NA_real_)) 

resultsGaussian <- loop_simulation(parameters)

```

# Ex : Gaussian constant measure bias

Results of each parameter for the case of SRS with **constant** measure bias, $\beta_{tel,age} > 0$ and Gaussian laws (with $\sigma =1$).

## Measure bias total

$$\text{relError} = \frac{1}{K}\sum_{k=1}^K \frac{\hat{t}_{\phi\Delta y,k}-t_{\phi\Delta y,k}}{\mathbb{E}[t_{\phi\Delta y}]}$$

$$\text{relRMSE} = \frac{\sqrt{\frac{1}{K}\sum_{k=1}^K (\hat{t}_{\phi\Delta y,k}-t_{\phi\Delta y,k})^2}}{|\mathbb{E}[t_{\phi\Delta y}]|}$$

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", 
         MBtype == "constant", signAge == "plus", 
         trueEstimator, sd == 1.0) %>% 
  select(-sampling, -MBtype, -probSelect) %>% 
  group_by(parameter, invMatrices, calculTotal)

temp %>% 
  summarise(K = n(),
            relError = round(mean(diffPhiBias / abs(expPhiBias)), 4L), 
            relRMSE = round(sqrt(mean(diffPhiBias^2L / expPhiBias^2L)), 4L)) %>% 
  ungroup() %>% 
  arrange(relRMSE)
```

The relative error `relError` is close to zero for each estimator. RMSEs are about the same, being in \[0.13, 0.18\]. For the best RMSE we have the model that consider the bias constant (up to a centered noise): the G-COMP model with a simple constant $\delta$ for the MB consideration.

## telephone total estimation

Our different estimators are all approximately unbiased, so we focus on the RMSE appearing when we estimate the total $t_2$ with each estimator:

```{r}
temp %>% 
  summarise(K = n(),
            relError = round(mean(diffYtelEstMP / abs(expYtel)), 4L),
            relBenchmarkError = round(mean(diffBenchmark / abs(expYtel)), 4L),
            relEstRMSE = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = mean(expSDTWBenchmark / abs(expYtel))) %>% 
  ungroup() %>% 
  arrange(relEstRMSE)

rm(temp)
```

The `relEstRMSEBenchmark` variable is the estimated RMSE of the benchmark estimator. We can observe that among the different estimators, we have a few of them that offer a **lower RMSE than the benchmark**. We have, with $\phi \equiv \frac{1}{2}$ :

-   G-COMPUTATION with 0, 1 or 2 degrees and a estimation of the total $\phi\Delta$ via the sample or the population

-   The HT Thompson estimator with sample inverse matrices and a total on the sample or the population

Summing the bias on exclusively $S_{r\bullet}$ seems to be effective (with `calculTotal = sample`), which is great because in that case we don't need to know the $z_k$ for units that didn't respond by internet.

The case $\phi = \frac{1}{N}, \frac{2}{N}, \cdots$ gives a RMSE higher than the benchmark RMSE.

With $\phi$ fixed, the other estimators give a RMSE nearly equal the benckmark.

The `relExpRMSETWBenchmark` variable is the expected standard deviation of the benchmark calculated with the true selection probabilities. The bias is equal to zero with the true weights so the RMSE is equal to the standard deviation. Biases of the estimators are nearly zero so RMSE and standard deviations can be compared. We can observe that for every estimator the (estimated) RMSE is **clearly lower** than the standard deviation / RMSE of the benchmark with true probabilities.

# Ex : Gaussian variable measure bias

Results of each parameter for the case of SRS with **variable** measure bias and Gaussian laws (and variable $\sigma$). First we focus on the bias:

```{r}
temp <- resultsGaussian %>% 
  filter(YtelLaw == "gaussian", YintLaw == "gaussian", 
         sampling == "SRS", MBtype == "variable", trueEstimator) %>% 
  select(-sampling, -MBtype, -probSelect) %>% 
  group_by(sd, signAge, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(sd, signAge) %>% 
  arrange(abs(relError), .by_group = TRUE) %>% 
  ungroup()

temp %>% 
  select(-relRMSEParam, -relEstRMSEBenchmark, -relExpRMSETWBenchmark)
```

We can observe that the relative estimated bias is close to zero. It increases with the conditional standard deviation of $Y_{tel}$ and $Y_{int}$ (`sd`).

Even with a variable measure bias, the G-COMP model with only a constant for the bias is approximately unbiased. It is due to the fact that we chose here a centered measure bias, respectively to the couple (age, sex).

And now the RMSE:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  group_by(sd) %>% 
  arrange(relRMSEParam, .by_group = TRUE)
```

It is better to have $\phi_k \equiv \phi$ compared to $\phi_k = \frac{k}{N}$ for any conditional variance of $Y_{tel}$ and $Y_{int}$ and any estimator.

The estimator HT with **complete** inverse matrices and summing the bias on the sample only is the greatest (in term of RMSE) for any value and the standard deviation. Its RMSE is better than the RMSE of the benchmark, which is itself better than the benchmark with true probabilities.

Summing on $S_{r\bullet}$ exclusively seems also working like with the constant case. More importantly the calculation on the sample only gives the greatest results for each value of $\sigma$.

Even if the G-COMp 0 degree model is unbiased, in every case it does not give efficient RMSE (always greater than the benchmark).

Only a few of the estimators give an interesting RMSE. Some of the others can give a RMSE about more than twice the RMSE of the benchmark. It is important to chose carefully the used estimator. On the other hand we can observe that the positive gain **is not really high** (in particular chen $\sigma$ is the highest, i.e. $\sigma = 5$) :

```{r}
temp %>%
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + geom_histogram(aes(x = ratioRMSE)) +
  ggtitle("Ratio RMSE parameter compared to RMSE benchmark")
```

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  group_by(sd) %>% 
  summarize(minRatioRMSE = min(relRMSEParam / relEstRMSEBenchmark),
            maxRatioRMSE = max(relRMSEParam / relEstRMSEBenchmark))
```

Estimators with a better RMSE than the benchmark that are the most present:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(parameter, invMatrices, calculTotal) %>% 
  summarize(n = n()) %>% 
  arrange(-n)
```

Three of the estimators are **exclusively based** on the data of the sample $S_{r\bullet}$.

And the parameters that offer the worse RMSE compared to the benchmark:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark, -relExpRMSETWBenchmark) %>% 
  slice_max(relRMSEParam, n = 5L)
```

```{r}
rm(temp)
```


## Impact of the standard deviation

We focus on the impact of the standard deviation. Is there estimators that give better RMSE than the benchmark, independently of $\sigma$?

```{r}
parameters <- expand_grid(constBias = TRUE,
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = "eq",
                          sdY = c(0.1, 1.0, 2.0, 5.0, 10.0, 20.0, 40.0, 60.0))

resultsSD <- loop_simulation(parameters, K = K)
```

```{r}
temp <- resultsSD %>% 
  filter(trueEstimator) %>% 
  select(-YintLaw, -YtelLaw, -sampling, -MBtype, -probSelect) %>% 
  group_by(signAge, sd, phi, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(signAge, sd) %>% 
  arrange(relRMSEParam, .by_group = TRUE) %>% 
  ungroup()
```

```{r}
temp %>% 
  group_by(signAge, sd) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark)
```
We can observe there is always estimators that are better than the benchmark, for any couple (`signAge`, `sdY`). In particular, the estimator `HT-samples-sample` is always present. Our estimators are still approximately unbiased but their variance increases with $\sigma$.

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + geom_point(aes(x = sd, y = ratioRMSE, color = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation")
```
Trajectories are similar for the both values of `signAge`, even if for small values of $\sigma$ the ratios are quite higher for `signAge = plus`. It seems that the ratio convergences to a value that is right under 1.

Best estimators depending on `signAge` and $\sigma$:

```{r}
temp %>% 
  select(-relError, -relErrorBenchmark) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  group_by(signAge, sd) %>% 
  slice_min(ratioRMSE) %>% 
  select(signAge, sd, phi, parameter, invMatrices, calculTotal, ratioRMSE)
```

The ratio is always right under one, which means that in those conditions ($\phi$ constant for instance), there are estimators that are better than the benchmark but not that much.

```{r}
temp %>% 
  group_by(signAge, sd) %>% 
  summarize(minRatioRMSE = min(relRMSEParam / relEstRMSEBenchmark),
            maxRatioRMSE = max(relRMSEParam / relEstRMSEBenchmark))
```


## Impact of $\phi$

We propose different values for $\phi$:

```{r}
parameters <- expand_grid(constBias = TRUE,
                          sampling = "SRS",
                          YtelLaw = "gaussian",
                          YintLaw = "gaussian",
                          signAge = c("plus", "minus"),
                          phi = c("eq", "1/3", "2/3",
                                  "var", "linear", "split3"),
                          sdY = c(1.0, 2.0, 5.0))

resultsPhi <- loop_simulation(parameters, K = K)
```

```{r}
temp <- resultsPhi %>% 
  filter(trueEstimator) %>% 
  select(-YintLaw, -YtelLaw, -sampling, -MBtype, -probSelect) %>% 
  group_by(signAge, sd, phi, parameter, invMatrices, calculTotal) %>% 
  summarise(#reldiffTotalTrueMean = round(mean(diffYtelTrueMP / abs(expYtel)) , 4L), 
            relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relErrorBenchmark = round(mean(diffBenchmark / abs(expYtel)), 4L),
            #relRMSETrue = round(sqrt(mean(diffYtelTrueMP^2L / expYtel^2L)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L),
            relExpRMSETWBenchmark = round(mean(expSDTWBenchmark / abs(expYtel)), 4L)) %>% 
  ungroup() %>% 
  group_by(signAge, sd, phi) %>% 
  arrange(relRMSEParam, .by_group = TRUE) %>% 
  ungroup()
```

List of the best(s) estimator(s) (for the RMSE) for each couple (`signAge`, `sd`):
```{r}
temp %>% 
  filter(relRMSEParam <= relExpRMSETWBenchmark) %>% 
  group_by(signAge, sd) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>%
  select(-relError, -relErrorBenchmark, 
         -relRMSEParam, -relEstRMSEBenchmark,
         -relExpRMSETWBenchmark) %>% 
  arrange(ratioRMSE) %>% 
  slice_min(ratioRMSE)
```

Evolution of the RMSE ratio depending on $\sigma$, $\phi$ and `signAge`:
```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + 
  geom_point(aes(x = sd, y = ratioRMSE, color = phi, shape = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation and phi")
```
`var` (i.e. a random choice for $\phi_k$) gives really bad results. So we will remove it from now on.

```{r}
temp %>% 
  filter(parameter == "HT", invMatrices == "samples", calculTotal == "sample") %>% 
  filter(phi != "var") %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  ggplot() + 
  geom_point(aes(x = sd, y = ratioRMSE, color = phi, shape = signAge)) +
  geom_hline(yintercept = 1.0, color = "red") +
  ggtitle("Ratio of the RMSE depending on the standard deviation and phi")
```
`split3` gives results that depends too much on $\beta_{tel}$ (unfortunately, because it gives good results in the case $\beta_{tel,age} < 0$). So we also shall remove it.
```{r}
temp %>% 
  filter(parameter == "HT",
         invMatrices == "samples",
         calculTotal == "sample",
         !phi %in% c("var", "split3")) %>% 
  mutate(ratioRMSE = relRMSEParam / relEstRMSEBenchmark) %>% 
  group_by(signAge, sd) %>% 
  select(phi, ratioRMSE) %>% 
  slice_min(ratioRMSE)
```
The choice of the constant depends of the situation. However, the difference between the ratios is not extremely important.

# More general cases

```{r}
#| eval: false

# Grid of all evaluated parameters
parameters <- expand_grid(constBias = c(TRUE, FALSE),
                          sampling = "SRS",#c("SRS", "STSRS"),
                          YtelLaw = c("gaussian", "exponential"),
                          YintLaw = c("gaussian", "exponential", "split3"),
                          signAge = c("plus", "minus"),
                          phi = c("eq", "var"),
                          sdY = c(1.0, 2.0, 5.0, "split3")) %>%
  # If there is only exponential laws the parameter sdY is of no interest
  mutate(sdY =
           ifelse(YtelLaw == "gaussian" |
                    YintLaw == "gaussian", sdY, NA_real_)) %>% 
  filter(YtelLaw != "gaussian" || YintLaw != "gaussian")

resultsGeneral <- loop_simulation(parameters)
```


## Telephone total estimation

The size of the grid of the hyperparameters (and so the number of cases) is huge, so each case cannot be studied individually.

Note : in the case of an exponential law for $Y_{int}$ or $Y_{tel}$, the standard deviation parameter is not considered (the standard deviation will be equal to $(z_k^T\beta)^2$, with the adequate $\beta$).

```{r}
temp <- resultsGeneral %>% 
  filter(trueEstimator) %>% 
  group_by(YintLaw, YtelLaw, sd, signAge, MBtype, phi,
           sampling, parameter, invMatrices, calculTotal)
```

Here we have the optimal relative RMSE for each condition:

```{r}
temp <- temp %>% 
  summarise(relError = round(mean(diffYtelEstMP / abs(expYtel)) , 4L),
            relDiffBenchmarkMean = round(mean(diffBenchmark / abs(expYtel)), 4L),
            relRMSEParam = round(sqrt(mean(diffYtelEstMP^2L / expYtel^2L)), 4L),
            relEstRMSEBenchmark = round(sqrt(mean(diffBenchmark^2L / expYtel^2L)), 4L)) %>% 
  ungroup() 

temp %>% 
  group_by(sd, YintLaw, YtelLaw, signAge, MBtype, sampling) %>% 
  slice_min(relRMSEParam)
```

Number of situations that possess a better RMSE with some parameter different than the benchmark:

```{r}
tempBis <- temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)

nbExperiments <- n_groups(tempBis)

nbSuccesses <- tempBis %>% 
  summarise(nbEstimators = sum(relRMSEParam <= relEstRMSEBenchmark),
            bestRelRMSE = min(relRMSEParam),
            relEstRMSEBenchmark = mean(relEstRMSEBenchmark)) %>% 
  arrange(bestRelRMSE / relEstRMSEBenchmark) %>% 
  filter(nbEstimators > 0L) %>% 
  nrow()
  
glue("{nbSuccesses} / {nbExperiments} with at least one interesting estimator")

rm(tempBis)
```

Estimators that are the most present (which have a lower value of RMSE compared to the benchmark, not necessarily the lowest):

```{r}
temp %>% 
  select(-relError, -relDiffBenchmarkMean) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  group_by(phi, parameter, invMatrices, calculTotal) %>% 
  summarize(n = n(), ratio = n / nbExperiments) %>% 
  arrange(-n)
```

Case when HT-sample-sample is the best:

```{r}
temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  slice_min(relRMSEParam) %>% 
  filter(parameter == "HT", 
         invMatrices == "samples",
         calculTotal == "sample") %>% 
  distinct(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)
```

This estimator seems to be optimal only is the measure bias is conditionally constant.

Case when HT-population-population is the best:

```{r}
temp %>% 
  group_by(YtelLaw, sd, YintLaw, sampling, signAge, MBtype) %>% 
  filter(relRMSEParam <= relEstRMSEBenchmark) %>% 
  slice_min(relRMSEParam) %>% 
  filter(parameter == "HT", 
         invMatrices == "population",
         calculTotal == "population") %>% 
  distinct(YtelLaw, sd, YintLaw, sampling, signAge, MBtype)
```


```{r}
rm(temp)
```

## Variances

Here we focus on the special case when selection probabilities are known and the estimator is Horvitz-Thompson with the complete inverse matrices and the summation for $\hat{t}_{\phi\Delta}$ is made on the entire population. The goal is to compare the estimated (co)variances with the ones expected by the theory.

```{r}
temp <- resultsGeneral %>% 
  filter(parameter == "HT", probSelect == "true", 
         invMatrices == "population", calculTotal == "population")


tempBis <- temp %>% 
  filter(sampling == "SRS",
         YintLaw == "gaussian", YtelLaw == "gaussian",
         phi == "eq", sd == 1.0, MBtype == "variable",
         signAge == "plus")
```

Before comparison, we can observe on the Gaussian case (variable measure bias, positive coefficient for age on telephone, $\phi_k \equiv \frac{1}{2}$) with $\sigma =1$ that the variance do not stabilize completely:

```{r}
sumSqErrors <- cumsum(tempBis[, c("diffPhiBias", "diffYtelEstMP")]^2L)

RMSEs <- 
  data.frame(totPhiDelta =  sumSqErrors[-1L, "diffPhiBias"] / Kvec[-1L],
             totYtel = sumSqErrors[-1L, "diffYtelEstMP"] / Kvec[-1L],
             K = Kvec[-1L])

RMSEs %>% 
  pivot_longer(cols = c("totPhiDelta", "totYtel"), 
               names_to = "target", 
               values_to = "estRMSE") %>% 
  ggplot() +
  geom_point(aes(x = K, y = estRMSE, colour = target)) +
  ggtitle("Estimated RMSE depending on the number of evaluations (K)")

rm(tempBis, RMSEs)
```


```{r}
temp %>% 
  group_by(sampling, YintLaw, YtelLaw, sd, MBtype, signAge, phi) %>% 
  summarise(K = n(),
            estSDEstimator = sd(estYtelTrueMP),
            expSDEstimator = sqrt(mean(expVar2)),
            estVarPhi1 = var(estPhiYintTrueMP),
            expVarPhi1 = mean(expVarPhi1),
            estVarPhi2 = var(estTotPhiYtelTrueMP),
            expVarPhi2 = mean(expVarPhi2),
            estCovarphi12 = cov(estPhiYintTrueMP, estTotPhiYtelTrueMP),
            expCovarPhi12 = mean(expCovarPhi12),
            estCovarphi1Delta = cov(estPhiYintTrueMP, estPhiBias),
            expCovarPhi1Delta = mean(expCovarPhi1Delta),
            estVarPhiDelta = var(estPhiBias),
            expVarPhiDelta = mean(expVarPhiDelta),
            estCovarphi2Delta = cov(estTotPhiYtelTrueMP, estPhiBias),
            expCovarPhi2Delta = mean(expCovarPhi2Delta))
```

```{r}
rm(temp)
```



